,YEAR,TITLE,ABSTRACT,AUTHOR,PUBLISHER,BOOKTITLE,CATEGORY,name,last_name,gender_g,gender_ize,gender_jur,concate_name
12039,2020,SCE}-{SUMMARY} at the {FNS} 2020 shared task,"With the constantly growing amount of information, the need arises to automatically summarize this written information. One of the challenges in the summary is that it{'}s difficult to generalize. For example, summarizing a news article is very different from summarizing a financial earnings report. This paper reports an approach for summarizing financial texts, which are different from the documents from other domains at least in three parameters: length, structure, and format. Our approach considers these parameters, it is adapted to hierarchical structure of sections, document length, and special {``}language{''}. The approach builds an hierarchical summary, visualized as a tree with summaries under different discourse topics. The approach was evaluated using extrinsic and intrinsic automated evaluations, which are reported in this paper. As all participants of the Financial Narrative Summarisation (FNS 2020) shared task, we used FNS2020 dataset for evaluations.","Litvak, Marina  and",COLING,Proceedings of the 1st Joint Workshop on Financial Narrative Processing and MultiLing Financial Summarisation,INPROCEEDINGS,Marina,Litvak,female,,female,Marina Litvak
12052,2020,Hierarchical summarization of financial reports with {RUNNER,"With the constantly growing amount of information, the need arises to automatically summarize this written information. One of the challenges in the summary is that it{'}s difficult to generalize. For example, summarizing a news article is very different from summarizing a financial earnings report. This paper reports an approach for summarizing financial texts, which are different from the documents from other domains at least in three parameters: length, structure, and format. Our approach considers these parameters, it is adapted to hierarchical structure of sections, document length, and special {``}language{''}. The approach builds an hierarchical summary, visualized as a tree with summaries under different discourse topics. The approach was evaluated using extrinsic and intrinsic automated evaluations, which are reported in this paper. As all participants of the Financial Narrative Summarisation (FNS 2020) shared task, we used FNS2020 dataset for evaluations.","Litvak, Marina  and",COLING,Proceedings of the 1st Joint Workshop on Financial Narrative Processing and MultiLing Financial Summarisation,INPROCEEDINGS,Marina,Litvak,female,,female,Marina Litvak
28899,2014,TAUS} post-editing course,"While there is a massive adoption of MT post-editing as a new service in the global translation industry, a common reference to skills and best practices to do this work well has been missing. TAUS took up the challenge to provide a course that would integrate with the DQF tools and the post-editing best practices developed by TAUS members in the previous years and offers both theory and practice to develop post-editing skills. The contribution of language service providers who are involved in MT and post-editing on a daily basis allowed TAUS to deliver fast on this industry need. This online course addresses the challenges for linguists and translators deciding to work on post-editing assignments and is aimed at those who want to learn the best practices and skills to become more efficient and proficient in the activity of post-editing.","G{\""o}r{\""o}g, Attila",Association for Machine Translation in the Americas,Proceedings of the 11th Conference of the Association for Machine Translation in the Americas,INPROCEEDINGS,Attila,g,male,,,
28900,2014,TAUS} post-editing productivity tool,"While there is a massive adoption of MT post-editing as a new service in the global translation industry, a common reference to skills and best practices to do this work well has been missing. TAUS took up the challenge to provide a course that would integrate with the DQF tools and the post-editing best practices developed by TAUS members in the previous years and offers both theory and practice to develop post-editing skills. The contribution of language service providers who are involved in MT and post-editing on a daily basis allowed TAUS to deliver fast on this industry need. This online course addresses the challenges for linguists and translators deciding to work on post-editing assignments and is aimed at those who want to learn the best practices and skills to become more efficient and proficient in the activity of post-editing.","G{\""o}r{\""o}g, Attila",Association for Machine Translation in the Americas,Proceedings of the 11th Conference of the Association for Machine Translation in the Americas,INPROCEEDINGS,Attila,g,male,,,
2101,2022,Uncovering Values: Detecting Latent Moral Content from Natural Language with Explainable and Non-Trained Methods,"Moral values as commonsense norms shape our everyday individual and community behavior. The possibility to extract moral attitude rapidly from natural language is an appealing perspective that would enable a deeper understanding of social interaction dynamics and the individual cognitive and behavioral dimension. In this work we focus on detecting moral content from natural language and we test our methods on a corpus of tweets previously labeled as containing moral values or violations, according to Moral Foundation Theory. We develop and compare two different approaches: (i) a frame-based symbolic value detector based on knowledge graphs and (ii) a zero-shot machine learning model fine-tuned on a task of Natural Language Inference (NLI) and a task of emotion detection. The final outcome from our work consists in two approaches meant to perform without the need for prior training process on a moral value detection task.","Asprino, Luigi  and",Association for Computational Linguistics,Proceedings of Deep Learning Inside Out (DeeLIO 2022): The 3rd Workshop on Knowledge Extraction and Integration for Deep Learning Architectures,INPROCEEDINGS,Luigi,Asprino,male,,,
2102,2022,Jointly Identifying and Fixing Inconsistent Readings from Information Extraction Systems,"Moral values as commonsense norms shape our everyday individual and community behavior. The possibility to extract moral attitude rapidly from natural language is an appealing perspective that would enable a deeper understanding of social interaction dynamics and the individual cognitive and behavioral dimension. In this work we focus on detecting moral content from natural language and we test our methods on a corpus of tweets previously labeled as containing moral values or violations, according to Moral Foundation Theory. We develop and compare two different approaches: (i) a frame-based symbolic value detector based on knowledge graphs and (ii) a zero-shot machine learning model fine-tuned on a task of Natural Language Inference (NLI) and a task of emotion detection. The final outcome from our work consists in two approaches meant to perform without the need for prior training process on a moral value detection task.","Padia, Ankur  and",Association for Computational Linguistics,Proceedings of Deep Learning Inside Out (DeeLIO 2022): The 3rd Workshop on Knowledge Extraction and Integration for Deep Learning Architectures,INPROCEEDINGS,Ankur,Padia,,male,,
16086,2019,Adversarial Attack on Sentiment Classification,"In this paper, we propose a white-box attack algorithm called {``}Global Search{''} method and compare it with a simple misspelling noise and a more sophisticated and common white-box attack approach called {``}Greedy Search{''}. The attack methods are evaluated on the Convolutional Neural Network (CNN) sentiment classifier trained on the IMDB movie review dataset. The attack success rate is used to evaluate the effectiveness of the attack methods and the perplexity of the sentences is used to measure the degree of distortion of the generated adversarial examples. The experiment results show that the proposed {``}Global Search{''} method generates more powerful adversarial examples with less distortion or less modification to the source text.","Tsai, Yi-Ting  and",Association for Computational Linguistics,Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP,INPROCEEDINGS,YiTing,Tsai,andy,,,
16438,2019,Adversarial Attack on Sentiment Classification,"In this paper, we propose a white-box attack algorithm called {``}Global Search{''} method and compare it with a simple misspelling noise and a more sophisticated and common white-box attack approach called {``}Greedy Search{''}. The attack methods are evaluated on the Convolutional Neural Network (CNN) sentiment classifier trained on the IMDB movie review dataset. The attack success rate is used to evaluate the effectiveness of the attack methods and the perplexity of the sentences is used to measure the degree of distortion of the generated adversarial examples. The experiment results show that the proposed {``}Global Search{''} method generates more powerful adversarial examples with less distortion or less modification to the source text.","Tsai, Yi-Ting (Alicia)  and",Association for Computational Linguistics,Proceedings of the 2019 Workshop on Widening NLP,INPROCEEDINGS,YiTing,Tsai,andy,,,
10968,2020,Content-Equivalent Translated Parallel News Corpus and Extension of Domain Adaptation for {NMT,"In this paper, we deal with two problems in Japanese-English machine translation of news articles. The first problem is the quality of parallel corpora. Neural machine translation (NMT) systems suffer degraded performance when trained with noisy data. Because there is no clean Japanese-English parallel data for news articles, we build a novel parallel news corpus consisting of Japanese news articles translated into English in a content-equivalent manner. This is the first content-equivalent Japanese-English news corpus translated specifically for training NMT systems. The second problem involves the domain-adaptation technique. NMT systems suffer degraded performance when trained with mixed data having different features, such as noisy data and clean data. Though the existing methods try to overcome this problem by using tags for distinguishing the differences between corpora, it is not sufficient. We thus extend a domain-adaptation method using multi-tags to train an NMT model effectively with the clean corpus and existing parallel news corpora with some types of noise. Experimental results show that our corpus increases the translation quality, and that our domain-adaptation method is more effective for learning with the multiple types of corpora than existing domain-adaptation methods are.","Mino, Hideya  and",European Language Resources Association,Proceedings of the 12th Language Resources and Evaluation Conference,INPROCEEDINGS,Hideya,Mino,,male,,
13970,2020,Effective Use of Target-side Context for Neural Machine Translation,"In this paper, we deal with two problems in Japanese-English machine translation of news articles. The first problem is the quality of parallel corpora. Neural machine translation (NMT) systems suffer degraded performance when trained with noisy data. Because there is no clean Japanese-English parallel data for news articles, we build a novel parallel news corpus consisting of Japanese news articles translated into English in a content-equivalent manner. This is the first content-equivalent Japanese-English news corpus translated specifically for training NMT systems. The second problem involves the domain-adaptation technique. NMT systems suffer degraded performance when trained with mixed data having different features, such as noisy data and clean data. Though the existing methods try to overcome this problem by using tags for distinguishing the differences between corpora, it is not sufficient. We thus extend a domain-adaptation method using multi-tags to train an NMT model effectively with the clean corpus and existing parallel news corpora with some types of noise. Experimental results show that our corpus increases the translation quality, and that our domain-adaptation method is more effective for learning with the multiple types of corpora than existing domain-adaptation methods are.","Mino, Hideya  and",International Committee on Computational Linguistics,Proceedings of the 28th International Conference on Computational Linguistics,INPROCEEDINGS,Hideya,Mino,,male,,
12282,2020,O}n the {I}nterplay {B}etween {F}ine-tuning and {S}entence-level {P}robing for {L}inguistic {K}nowledge in {P}re-trained {T}ransformers,"Fine-tuning pre-trained contextualized embedding models has become an integral part of the NLP pipeline. At the same time, probing has emerged as a way to investigate the linguistic knowledge captured by pre-trained models. Very little is, however, understood about how fine-tuning affects the representations of pre-trained models and thereby the linguistic knowledge they encode. This paper contributes towards closing this gap. We study three different pre-trained models: BERT, RoBERTa, and ALBERT, and investigate through sentence-level probing how fine-tuning affects their representations. We find that for some probing tasks fine-tuning leads to substantial changes in accuracy, possibly suggesting that fine-tuning introduces or even removes linguistic knowledge from a pre-trained model. These changes, however, vary greatly across different models, fine-tuning and probing tasks. Our analysis reveals that while fine-tuning indeed changes the representations of a pre-trained model and these changes are typically larger for higher layers, only in very few cases, fine-tuning has a positive effect on probing accuracy that is larger than just using the pre-trained model with a strong pooling method. Based on our findings, we argue that both positive and negative effects of fine-tuning on probing require a careful interpretation.","Mosbach, Marius  and",Association for Computational Linguistics,Findings of the Association for Computational Linguistics: EMNLP 2020,INPROCEEDINGS,Marius,Mosbach,male,,,
14497,2020,On the Interplay Between Fine-tuning and Sentence-Level Probing for Linguistic Knowledge in Pre-Trained Transformers,"Fine-tuning pre-trained contextualized embedding models has become an integral part of the NLP pipeline. At the same time, probing has emerged as a way to investigate the linguistic knowledge captured by pre-trained models. Very little is, however, understood about how fine-tuning affects the representations of pre-trained models and thereby the linguistic knowledge they encode. This paper contributes towards closing this gap. We study three different pre-trained models: BERT, RoBERTa, and ALBERT, and investigate through sentence-level probing how fine-tuning affects their representations. We find that for some probing tasks fine-tuning leads to substantial changes in accuracy, possibly suggesting that fine-tuning introduces or even removes linguistic knowledge from a pre-trained model. These changes, however, vary greatly across different models, fine-tuning and probing tasks. Our analysis reveals that while fine-tuning indeed changes the representations of a pre-trained model and these changes are typically larger for higher layers, only in very few cases, fine-tuning has a positive effect on probing accuracy that is larger than just using the pre-trained model with a strong pooling method. Based on our findings, we argue that both positive and negative effects of fine-tuning on probing require a careful interpretation.","Mosbach, Marius  and",Association for Computational Linguistics,Proceedings of the Third BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP,INPROCEEDINGS,Marius,Mosbach,male,,,
