{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Scraping_ACL.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mateosuster/nlpPapers/blob/main/code/02.Scraping_ACL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yeMr53Bg5mAW"
      },
      "outputs": [],
      "source": [
        "# Para hacer pedidos mediante el protocolo HTTP \n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import re\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://nips.cc/ "
      ],
      "metadata": {
        "id": "ZDkQIuzOQiia"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Vamos a jugar un poco con la pagina de Exactas\n",
        "url_base = 'https://aclanthology.org/events/acl-2020/#2020-nlpmc-1'\n",
        "\n",
        "html_obtenido = requests.get(url_base)\n",
        "soup = BeautifulSoup(html_obtenido.text, \"html.parser\")\n",
        "\n",
        "print(type(soup))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aC9IxwY354qB",
        "outputId": "080a3fd8-e9b7-4460-c46c-75ed347bb74f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'bs4.BeautifulSoup'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(soup)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bg-W4FiV6ige",
        "outputId": "eb585480-449e-4dcf-f02d-bede481a0f59"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "IOPub data rate exceeded.\n",
            "The notebook server will temporarily stop sending output\n",
            "to the client in order to avoid crashing it.\n",
            "To change this limit, set the config variable\n",
            "`--NotebookApp.iopub_data_rate_limit`.\n",
            "\n",
            "Current values:\n",
            "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
            "NotebookApp.rate_limit_window=3.0 (secs)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(soup.prettify())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pa4Ih68j6ime",
        "outputId": "b38229e4-02bd-42bc-da5c-6d6a8a5c818c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "IOPub data rate exceeded.\n",
            "The notebook server will temporarily stop sending output\n",
            "to the client in order to avoid crashing it.\n",
            "To change this limit, set the config variable\n",
            "`--NotebookApp.iopub_data_rate_limit`.\n",
            "\n",
            "Current values:\n",
            "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
            "NotebookApp.rate_limit_window=3.0 (secs)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# El método \"find_all\" busca TODOS los elementos de la pagina con ese tag y devuelve una lista que los contiene (en realidad devuelve un objeto de la clase \"bs4.element.ResultSet\")\n",
        "p_todos = soup.find_all('p')\n",
        "print(p_todos)\n",
        "\n"
      ],
      "metadata": {
        "id": "yzG7yAnD7GHq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "abstracts = soup.find_all('div', class_ = \"card-body p-3 small\")\n",
        "abs_lis = []\n",
        "for abstract in abstracts:\n",
        "#  print(abstract.text)\n",
        " abs_lis.append(abstract.text)\n",
        "\n",
        "abs_lis"
      ],
      "metadata": {
        "id": "pOY_PSgF9AjY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(abstracts)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rcGDJmrsA_dX",
        "outputId": "209bfe93-34cb-417b-99d1-1210069aaf67"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1238"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "abs_lis[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "e7eh0eRWA4eg",
        "outputId": "567945d2-31eb-4d78-cd94-913bb8dff95d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Speech directed to children differs from adult-directed speech in linguistic aspects such as repetition, word choice, and sentence length, as well as in aspects of the speech signal itself, such as prosodic and phonemic variation. Human language acquisition research indicates that child-directed speech helps language learners. This study explores the effect of child-directed speech when learning to extract semantic information from speech directly. We compare the task performance of models trained on adult-directed speech (ADS) and child-directed speech (CDS). We find indications that CDS helps in the initial stages of learning, but eventually, models trained on ADS reach comparable task performance, and generalize better. The results suggest that this is at least partially due to linguistic rather than acoustic properties of the two registers, as we see the same pattern when looking at models trained on acoustically comparable synthetic speech.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# titles = soup.find_all('span', class_ = \"d-block\")\n",
        "# titles[4]"
      ],
      "metadata": {
        "id": "cuOQcxW09caF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "titles = soup.find_all('strong')\n",
        "titles[1262]\n",
        "# len(titles)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LgFE4SE8-kmG",
        "outputId": "6f22650d-3d50-49f5-9fab-aebf6337d649"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<strong><a class=\"align-middle\" href=\"/2020.winlp-1.41/\">Enhanced <span class=\"acl-fixed-case\">U</span>rdu Word Segmentation using Conditional Random Fields and Morphological Context Features</a></strong>"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ngdNujIjFIEn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tit_lis = []\n",
        "for title in titles:\n",
        "#  print(abstract.text)\n",
        " tit_lis.append(title.text)\n",
        "\n"
      ],
      "metadata": {
        "id": "Utzq4UEf-zAn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mugre = '^Proceedings of the '\n",
        "\n",
        "tit_lis.count(mugre)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yc58809dDEtp",
        "outputId": "c7b54f6d-b2cc-4bc2-fa53-f752c4a1d7cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocesing(x):\n",
        "  x_clean = []\n",
        "  contador = 0\n",
        "  for i in x:\n",
        "    rtr = re.findall(r'^Proceedings of *|Second Grand-Challenge', i) \n",
        "    if rtr != []:\n",
        "      contador += 1\n",
        "      x_clean.append(rtr)\n",
        "  \n",
        "  return x_clean, contador\n",
        "\n",
        "preprocesing(tit_lis)"
      ],
      "metadata": {
        "id": "dVRQlq-MEPcB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(titles) - len(abstracts) - preprocesing(tit_lis)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xTjOELWlFI6s",
        "outputId": "df2dd88c-c934-40c3-b938-41b42c902f53"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "titles[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cqp1nmpGFXU-",
        "outputId": "13601a6b-6b1b-4443-dd6b-646e06c95dc0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<strong><a class=\"align-middle\" href=\"/2020.acl-main.0/\">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</a></strong>"
            ]
          },
          "metadata": {},
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tit_lis.count('Proceedings of the')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "INmfze_KEtwX",
        "outputId": "fa060579-6285-4963-afd5-edd6d46c4bd7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "for i in tit_lis:\n",
        "  print(re.findall(r'^Proceedings of the *', i) )"
      ],
      "metadata": {
        "id": "oTjbFTrJDkAQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tit_lis[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "uuX_6Ff6Chg_",
        "outputId": "c97cc0b4-2529-473b-b9df-a5184ca7396b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Learning to Understand Child-directed and Adult-directed Speech'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pd.DataFrame( {'abstract' : abs_lis})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "QlebZ2wAI3f_",
        "outputId": "a15602ef-ea7b-41b1-96b9-960572951f60"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                               abstract\n",
              "0     Speech directed to children differs from adult...\n",
              "1     Accurately diagnosing depression is difficult–...\n",
              "2     As an essential task in task-oriented dialog s...\n",
              "3     Automatic dialogue response evaluator has been...\n",
              "4     Recent proposed approaches have made promising...\n",
              "...                                                 ...\n",
              "1233  This abstract presents preliminary work in the...\n",
              "1234  Neural Machine Translation (NMT) for low-resou...\n",
              "1235  The Internet is frequently used as a platform ...\n",
              "1236  Byte-Pair Encoding (BPE) (Sennrich et al., 201...\n",
              "1237  Word segmentation is a fundamental task for mo...\n",
              "\n",
              "[1238 rows x 1 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ea2ff78e-8285-4e84-9306-b4cfe951c391\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>abstract</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Speech directed to children differs from adult...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Accurately diagnosing depression is difficult–...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>As an essential task in task-oriented dialog s...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Automatic dialogue response evaluator has been...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Recent proposed approaches have made promising...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1233</th>\n",
              "      <td>This abstract presents preliminary work in the...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1234</th>\n",
              "      <td>Neural Machine Translation (NMT) for low-resou...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1235</th>\n",
              "      <td>The Internet is frequently used as a platform ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1236</th>\n",
              "      <td>Byte-Pair Encoding (BPE) (Sennrich et al., 201...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1237</th>\n",
              "      <td>Word segmentation is a fundamental task for mo...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1238 rows × 1 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ea2ff78e-8285-4e84-9306-b4cfe951c391')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-ea2ff78e-8285-4e84-9306-b4cfe951c391 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-ea2ff78e-8285-4e84-9306-b4cfe951c391');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 96
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# pd.DataFrame( {'abstract' : abs_lis, 'title': tit_lis})\n",
        "\n",
        "data = pd.DataFrame( {'title': tit_lis})#.to_csv('titles.csv')\n",
        "\n",
        "a=data[~data.title.str.contains('^Proceedings*|Second Grand-Challenge', regex= True, na=False)]\n",
        "for i in a.title:\n",
        "  print(i)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Qmv6crRBuNc",
        "outputId": "22b61d3b-69e1-4b6f-fd19-5786fca753ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Learning to Understand Child-directed and Adult-directed Speech\n",
            "Predicting Depression in Screening Interviews from Latent Categorization of Interview Prompts\n",
            "Coach: A Coarse-to-Fine Approach for Cross-domain Slot Filling\n",
            "Designing Precise and Robust Dialogue Response Evaluators\n",
            "Dialogue State Tracking with Explicit Slot Connection Modeling\n",
            "Generating Informative Conversational Response using Recurrent Knowledge-Interaction and Knowledge-Copy\n",
            "Guiding Variational Response Generator to Exploit Persona\n",
            "Large Scale Multi-Actor Generative Dialog Modeling\n",
            "PLATO: Pre-trained Dialogue Generation Model with Discrete Latent Variable\n",
            "Slot-consistent NLG for Task-oriented Dialogue Systems with Iterative Rectification Network\n",
            "Span-ConveRT: Few-shot Span Extraction for Dialog with Pretrained Conversational Representations\n",
            "Zero-Shot Transfer Learning with Synthesized Data for Multi-Domain Dialogue State Tracking\n",
            "A Complete Shift-Reduce Chinese Discourse Parser with Robust Dynamic Oracle\n",
            "TransS-Driven Joint Learning Architecture for Implicit Discourse Relation Recognition\n",
            "A Study of Non-autoregressive Model for Sequence Generation\n",
            "Cross-modal Language Generation using Pivot Stabilization for Web-scale Language Coverage\n",
            "Fact-based Text Editing\n",
            "Few-Shot NLG with Pre-Trained Language Model\n",
            "Fluent Response Generation for Conversational Question Answering\n",
            "Generating Diverse and Consistent QA pairs from Contexts with Information-Maximizing Hierarchical Conditional VAEs\n",
            "Learning to Ask More: Semi-Autoregressive Sequential Question Generation under Dual-Graph Interaction\n",
            "Neural Syntactic Preordering for Controlled Paraphrase Generation\n",
            "Pre-train and Plug-in: Flexible Conditional Text Generation with Variational Auto-Encoders\n",
            "Probabilistically Masked Language Model Capable of Autoregressive Generation in Arbitrary Word Order\n",
            "Reverse Engineering Configurations of Neural Text Generation Models\n",
            "Review-based Question Generation with Adaptive Instance Transfer and Augmentation\n",
            "TAG : Type Auxiliary Guiding for Code Comment Generation\n",
            "Unsupervised Paraphrasing by Simulated Annealing\n",
            "A Joint Model for Document Segmentation and Segment Labeling\n",
            "Contextualized Weak Supervision for Text Classification\n",
            "Every Document Owns Its Structure: Inductive Text Classification via Graph Neural Networks\n",
            "Neural Topic Modeling with Bidirectional Adversarial Training\n",
            "Text Classification with Negative Supervision\n",
            "Content Word Aware Neural Machine Translation\n",
            "Evaluating Explanation Methods for Neural Machine Translation\n",
            "Jointly Masked Sequence-to-Sequence Model for Non-Autoregressive Neural Machine Translation\n",
            "Learning Source Phrase Representations for Neural Machine Translation\n",
            "Lipschitz Constrained Parameter Initialization for Deep Transformers\n",
            "Location Attention for Extrapolation to Longer Sequences\n",
            "Multiscale Collaborative Deep Models for Neural Machine Translation\n",
            "Norm-Based Curriculum Learning for Neural Machine Translation\n",
            "Opportunistic Decoding with Timely Correction for Simultaneous Translation\n",
            "A Formal Hierarchy of RNN Architectures\n",
            "A Three-Parameter Rank-Frequency Relation in Natural Languages\n",
            "Dice Loss for Data-imbalanced NLP Tasks\n",
            "Emergence of Syntax Needs Minimal Supervision\n",
            "Language Models as an Alternative Evaluator of Word Order Hypotheses: A Case Study in Japanese\n",
            "GCAN: Graph-aware Co-Attention Networks for Explainable Fake News Detection on Social Media\n",
            "Integrating Semantic and Structural Information with Graph Convolutional Network for Controversy Detection\n",
            "Predicting the Topical Stance and Political Leaning of Media using Tweets\n",
            "Simple, Interpretable and Stable Method for Detecting Words with Usage Change across Corpora\n",
            "CDL: Curriculum Dual Learning for Emotion-Controllable Response Generation\n",
            "Efficient Dialogue State Tracking by Selectively Overwriting Memory\n",
            "End-to-End Neural Pipeline for Goal-Oriented Dialogue Systems using GPT-2\n",
            "Evaluating Dialogue Generation Systems via Response Selection\n",
            "Gated Convolutional Bidirectional Attention-based Model for Off-topic Spoken Response Detection\n",
            "Learning Low-Resource End-To-End Goal-Oriented Dialog for Fast and Reliable System Deployment\n",
            "Learning to Tag OOV Tokens by Integrating Contextual Representation and Background Knowledge\n",
            "Multi-Agent Task-Oriented Dialog Policy Learning with Role-Aware Reward Decomposition\n",
            "Paraphrase Augmented Task-Oriented Dialog Generation\n",
            "Response-Anticipated Memory for On-Demand Knowledge Integration in Response Generation\n",
            "Semi-Supervised Dialogue Policy Learning via Stochastic Reward Estimation\n",
            "Towards Unsupervised Language Understanding and Generation by Joint Dual Learning\n",
            "USR: An Unsupervised and Reference Free Evaluation Metric for Dialog Generation\n",
            "Explicit Semantic Decomposition for Definition Generation\n",
            "Improved Natural Language Generation via Loss Truncation\n",
            "Line Graph Enhanced AMR-to-Text Generation with Mix-Order Graph Attention Networks\n",
            "Rigid Formats Controlled Text Generation\n",
            "Syn-QG: Syntactic and Shallow Semantic Rules for Question Generation\n",
            "An Online Semantic-enhanced Dirichlet Model for Short Text Stream Clustering\n",
            "Generative Semantic Hashing Enhanced via Boltzmann Machines\n",
            "Interactive Construction of User-Centric Dictionary for Text Analytics\n",
            "Tree-Structured Neural Topic Model\n",
            "Unsupervised FAQ Retrieval with Question Generation and BERT\n",
            "“The Boating Store Had Its Best Sail Ever”: Pronunciation-attentive Contextualized Pun Recognition\n",
            "Fast and Accurate Deep Bidirectional Language Representations for Unsupervised Learning\n",
            "Fine-grained Interest Matching for Neural News Recommendation\n",
            "Interpretable Operational Risk Classification with Semi-Supervised Variational Autoencoder\n",
            "Interpreting Twitter User Geolocation\n",
            "Modeling Code-Switch Languages Using Bilingual Parallel Corpus\n",
            "SpellGCN: Incorporating Phonological and Visual Similarities into Language Models for Chinese Spelling Check\n",
            "Spelling Error Correction with Soft-Masked BERT\n",
            "A Frame-based Sentence Representation for Machine Reading Comprehension\n",
            "A Methodology for Creating Question Answering Corpora Using Inverse Data Annotation\n",
            "Contextualized Sparse Representations for Real-Time Open-Domain Question Answering\n",
            "Dynamic Sampling Strategies for Multi-Task Reading Comprehension\n",
            "Enhancing Answer Boundary Detection for Multilingual Machine Reading Comprehension\n",
            "Explicit Memory Tracker with Coarse-to-Fine Reasoning for Conversational Machine Reading\n",
            "Injecting Numerical Reasoning Skills into Language Models\n",
            "Learning to Identify Follow-Up Questions in Conversational Question Answering\n",
            "Query Graph Generation for Answering Multi-hop Complex Questions from Knowledge Bases\n",
            "A Diverse Corpus for Evaluating and Developing English Math Word Problem Solvers\n",
            "Improving Image Captioning Evaluation by Considering Inter References Variance\n",
            "Revisiting the Context Window for Cross-lingual Word Embeddings\n",
            "Moving Down the Long Tail of Word Sense Disambiguation with Gloss Informed Bi-encoders\n",
            "Code-Switching Patterns Can Be an Effective Route to Improve Performance of Downstream NLP Applications: A Case Study of Humour, Sarcasm and Hate Speech Detection\n",
            "DTCA: Decision Tree-based Co-Attention Networks for Explainable Claim Verification\n",
            "Towards Conversational Recommendation over Multi-Type Dialogs\n",
            "Unknown Intent Detection Using Gaussian Mixture Model with an Application to Zero-shot Intent Classification\n",
            "Expertise Style Transfer: A New Task Towards Better Communication between Experts and Laymen\n",
            "Towards Faithful Neural Table-to-Text Generation with Content-Matching Constraints\n",
            "Dynamic Memory Induction Networks for Few-Shot Text Classification\n",
            "Exclusive Hierarchical Decoding for Deep Keyphrase Generation\n",
            "Hierarchy-Aware Global Model for Hierarchical Text Classification\n",
            "Keyphrase Generation for Scientific Document Retrieval\n",
            "A Graph Auto-encoder Model of Derivational Morphology\n",
            "Building a User-Generated Content North-African Arabizi Treebank: Tackling Hell\n",
            "Crawling and Preprocessing Mailing Lists At Scale for Dialog Analysis\n",
            "Fine-Grained Analysis of Cross-Linguistic Syntactic Divergences\n",
            "Generating Counter Narratives against Online Hate Speech: Data and Strategies\n",
            "KLEJ: Comprehensive Benchmark for Polish Language Understanding\n",
            "Learning and Evaluating Emotion Lexicons for 91 Languages\n",
            "Multi-Hypothesis Machine Translation Evaluation\n",
            "Multimodal Quality Estimation for Machine Translation\n",
            "PuzzLing Machines: A Challenge on Learning From Small Data\n",
            "The SOFC-Exp Corpus and Neural Approaches to Information Extraction in the Materials Science Domain\n",
            "The TechQA Dataset\n",
            "iSarcasm: A Dataset of Intended Sarcasm\n",
            "AMR Parsing via Graph-Sequence Iterative Inference\n",
            "A Large-Scale Multi-Document Summarization Dataset from the Wikipedia Current Events Portal\n",
            "Attend, Translate and Summarize: An Efficient Method for Neural Cross-Lingual Summarization\n",
            "Examining the State-of-the-Art in News Timeline Summarization\n",
            "Improving Truthfulness of Headline Generation\n",
            "SUPERT: Towards New Frontiers in Unsupervised Evaluation Metrics for Multi-Document Summarization\n",
            "Self-Attention Guided Copy Mechanism for Abstractive Summarization\n",
            "Beyond User Self-Reported Likert Scale Ratings: A Comparison Model for Automatic Dialog Evaluation\n",
            "Conversational Word Embedding for Retrieval-Based Dialog System\n",
            "Few-shot Slot Tagging with Collapsed Dependency Transfer and Label-enhanced Task-adaptive Projection Network\n",
            "Learning Dialog Policies from Weak Demonstrations\n",
            "MuTual: A Dataset for Multi-Turn Dialogue Reasoning\n",
            "You Impress Me: Dialogue Generation via Mutual Persona Perception\n",
            "Bridging Anaphora Resolution as Question Answering\n",
            "Dialogue Coherence Assessment Without Explicit Dialogue Act Labels\n",
            "Fast and Accurate Non-Projective Dependency Tree Linearization\n",
            "Semantic Graphs for Generating Deep Questions\n",
            "A Novel Cascade Binary Tagging Framework for Relational Triple Extraction\n",
            "In Layman’s Terms: Semi-Open Relation Extraction from Scientific Texts\n",
            "NAT: Noise-Aware Training for Robust Neural Sequence Labeling\n",
            "Named Entity Recognition without Labelled Data: A Weak Supervision Approach\n",
            "Probing Linguistic Features of Sentence-Level Representations in Neural Relation Extraction\n",
            "Reasoning with Latent Structure Refinement for Document-Level Relation Extraction\n",
            "TACRED Revisited: A Thorough Evaluation of the TACRED Relation Extraction Task\n",
            "Bilingual Dictionary Based Neural Machine Translation without Using Parallel Sentences\n",
            "Boosting Neural Machine Translation with Similar Translations\n",
            "Character-Level Translation with Self-attention\n",
            "End-to-End Neural Word Alignment Outperforms GIZA++\n",
            "Enhancing Machine Translation with Dependency-Aware Self-Attention\n",
            "Improving Massively Multilingual Neural Machine Translation and Zero-Shot Translation\n",
            "It’s Easier to Translate out of English than into it: Measuring Neural Translation Difficulty by Cross-Mutual Information\n",
            "Language-aware Interlingua for Multilingual Neural Machine Translation\n",
            "On the Limitations of Cross-lingual Encoders as Exposed by Reference-Free Machine Translation Evaluation\n",
            "Parallel Sentence Mining by Constrained Decoding\n",
            "Self-Attention with Cross-Lingual Position Representation\n",
            "“You Sound Just Like Your Father” Commercial Machine Translation Systems Include Stylistic Biases\n",
            "MMPE: A Multi-Modal Interface for Post-Editing Machine Translation\n",
            "A Monolingual Approach to Contextualized Word Embeddings for Mid-Resource Languages\n",
            "Will-They-Won’t-They: A Very Large Dataset for Stance Detection on Twitter\n",
            "A Systematic Assessment of Syntactic Generalization in Neural Language Models\n",
            "Inflecting When There’s No Majority: Limitations of Encoder-Decoder Neural Networks as Cognitive Models for German Plurals\n",
            "Overestimation of Syntactic Representation in Neural Language Models\n",
            "Modelling Suspense in Short Stories as Uncertainty Reduction over Neural Representation\n",
            "You Don’t Have Time to Read This: An Exploration of Document Reading Time Prediction\n",
            "A Generative Model for Joint Natural Language Understanding and Generation\n",
            "Automatic Detection of Generated Text is Easiest when Humans are Fooled\n",
            "Multi-Domain Neural Machine Translation with Word-Level Adaptive Layer-wise Domain Mixing\n",
            "Conversational Graph Grounded Policy Learning for Open-Domain Conversation Generation\n",
            "GPT-too: A Language-Model-First Approach for AMR-to-Text Generation\n",
            "Learning to Update Natural Language Comments Based on Code Changes\n",
            "Politeness Transfer: A Tag and Generate Approach\n",
            "BPE-Dropout: Simple and Effective Subword Regularization\n",
            "Improving Non-autoregressive Neural Machine Translation with Monolingual Data\n",
            "Attend to Medical Ontologies: Content Selection for Clinical Abstractive Summarization\n",
            "On Faithfulness and Factuality in Abstractive Summarization\n",
            "Screenplay Summarization Using Latent Narrative Structure\n",
            "Unsupervised Opinion Summarization with Noising and Denoising\n",
            "A Tale of Two Perplexities: Sensitivity of Neural Language Models to Lexical Retrieval Deficits in Dementia of the Alzheimer’s Type\n",
            "Probing Linguistic Systematicity\n",
            "Recollection versus Imagination: Exploring Human Memory and Cognition via Neural Language Models\n",
            "Recurrent Neural Network Language Models Always Learn English-Like Relative Clause Attachment\n",
            "Speakers enhance contextually confusable words\n",
            "What determines the order of adjectives in English? Comparing efficiency-based theories using dependency treebanks\n",
            "“None of the Above”: Measure Uncertainty in Dialog Response Retrieval\n",
            "Can You Put it All Together: Evaluating Conversational Agents’ Ability to Blend Skills\n",
            "Grounded Conversation Generation as Guided Traverses in Commonsense Knowledge Graphs\n",
            "Negative Training for Neural Dialogue Response Generation\n",
            "Recursive Template-based Frame Generation for Task Oriented Dialog\n",
            "Speak to your Parser: Interactive Text-to-SQL with Natural Language Feedback\n",
            "Calibrating Structured Output Predictors for Natural Language Processing\n",
            "Active Imitation Learning with Noisy Guidance\n",
            "ExpBERT: Representation Engineering with Natural Language Explanations\n",
            "GAN-BERT: Generative Adversarial Learning for Robust Text Classification with a Bunch of Labeled Examples\n",
            "Generalizing Natural Language Analysis through Span-relation Representations\n",
            "Learning to Contextually Aggregate Multi-Source Supervision for Sequence Labeling\n",
            "MixText: Linguistically-Informed Interpolation of Hidden Space for Semi-Supervised Text Classification\n",
            "MobileBERT: a Compact Task-Agnostic BERT for Resource-Limited Devices\n",
            "On Importance Sampling-Based Evaluation of Latent Language Models\n",
            "SMART: Robust and Efficient Fine-Tuning for Pre-trained Natural Language Models through Principled Regularized Optimization\n",
            "Stolen Probability: A Structural Weakness of Neural Language Models\n",
            "Taxonomy Construction of Unseen Domains via Graph-based Cross-Domain Knowledge Transfer\n",
            "To Pretrain or Not to Pretrain: Examining the Benefits of Pretrainng on Resource Rich Tasks\n",
            "Why Overfitting Isn’t Always Bad: Retrofitting Cross-Lingual Word Embeddings to Dictionaries\n",
            "XtremeDistil: Multi-stage Distillation for Massive Multilingual Models\n",
            "A Girl Has A Name: Detecting Authorship Obfuscation\n",
            "DeeBERT: Dynamic Early Exiting for Accelerating BERT Inference\n",
            "Efficient Strategies for Hierarchical Text Classification: External Knowledge and Auxiliary Tasks\n",
            "Investigating the effect of auxiliary objectives for the automated grading of learner English speech transcriptions\n",
            "SPECTER: Document-level Representation Learning using Citation-informed Transformers\n",
            "Semantic Scaffolds for Pseudocode-to-Code Generation\n",
            "Can We Predict New Facts with Open Knowledge Graph Embeddings? A Benchmark for Open Link Prediction\n",
            "INFOTABS: Inference on Tables as Semi-structured Data\n",
            "Interactive Machine Comprehension with Information Seeking Agents\n",
            "Syntactic Data Augmentation Increases Robustness to Inference Heuristics\n",
            "Improved Speech Representations with Multi-Target Autoregressive Predictive Coding\n",
            "Integrating Multimodal Information in Large Pretrained Transformers\n",
            "MultiQT: Multimodal learning for real-time question tracking in speech\n",
            "Multimodal and Multiresolution Speech Recognition with Transformers\n",
            "Phone Features Improve Speech Translation\n",
            "Grounding Conversations with Improvised Dialogues\n",
            "Image-Chat: Engaging Grounded Conversations\n",
            "Learning an Unreferenced Metric for Online Dialogue Evaluation\n",
            "Neural Generation of Dialogue Response Timings\n",
            "The Dialogue Dodecathlon: Open-Domain Knowledge and Image Grounded Conversational Agents\n",
            "Automatic Poetry Generation from Prosaic Text\n",
            "Bridging the Structural Gap Between Encoding and Decoding for Data-To-Text Generation\n",
            "Enabling Language Models to Fill in the Blanks\n",
            "INSET: Sentence Infilling with INter-SEntential Transformer\n",
            "Improving Adversarial Text Generation by Modeling the Distant Future\n",
            "Simple and Effective Retrieve-Edit-Rerank Text Generation\n",
            "BabyWalk: Going Farther in Vision-and-Language Navigation by Taking Baby Steps\n",
            "Cross-media Structured Common Space for Multimedia Event Extraction\n",
            "Learning to Segment Actions from Observation and Narration\n",
            "Learning to execute instructions in a Minecraft dialogue\n",
            "MART: Memory-Augmented Recurrent Transformer for Coherent Video Paragraph Captioning\n",
            "What is Learned in Visually Grounded Neural Syntax Acquisition\n",
            "A Batch Normalized Inference Network Keeps the KL Vanishing Away\n",
            "Contextual Embeddings: When Are They Worth It?\n",
            "Interactive Classification by Asking Informative Questions\n",
            "Knowledge Graph Embedding Compression\n",
            "Low Resource Sequence Tagging using Sentence Reconstruction\n",
            "Masked Language Model Scoring\n",
            "Orthogonal Relation Transforms with Graph Context Modeling for Knowledge Graph Embedding\n",
            "Posterior Calibrated Training on Sentence Classification Tasks\n",
            "Posterior Control of Blackbox Generation\n",
            "Pretrained Transformers Improve Out-of-Distribution Robustness\n",
            "Robust Encodings: A Framework for Combating Adversarial Typos\n",
            "Showing Your Work Doesn’t Always Work\n",
            "Span Selection Pre-training for Question Answering\n",
            "Topological Sort for Sentence Ordering\n",
            "Weight Poisoning Attacks on Pretrained Models\n",
            "schuBERT: Optimizing Elements of BERT\n",
            "ENGINE: Energy-Based Inference Networks for Non-Autoregressive Machine Translation\n",
            "Leveraging Monolingual Data with Self-Supervision for Multilingual Neural Machine Translation\n",
            "On The Evaluation of Machine Translation Systems Trained With Back-Translation\n",
            "Simultaneous Translation Policies: From Fixed to Adaptive\n",
            "Breaking Through the 80% Glass Ceiling: Raising the State of the Art in Word Sense Disambiguation by Incorporating Knowledge Graph Information\n",
            "Glyph2Vec: Learning Chinese Out-of-Vocabulary Word Embedding from Glyphs\n",
            "Multidirectional Associative Optimization of Function-Specific Word Representations\n",
            "Predicting Degrees of Technicality in Automatic Terminology Extraction\n",
            "Verbal Multiword Expressions for Identification of Metaphor\n",
            "Gender Bias in Multilingual Embeddings and Cross-Lingual Transfer\n",
            "Give Me Convenience and Give Her Death: Who Should Decide What Uses of NLP are Appropriate, and on What Basis?\n",
            "Is Your Classifier Actually Biased? Measuring Fairness under Uncertainty with Bernstein Bounds\n",
            "It’s Morphin’ Time! Combating Linguistic Discrimination with Inflectional Perturbations\n",
            "Mitigating Gender Bias Amplification in Distribution by Posterior Regularization\n",
            "Towards Understanding Gender Bias in Relation Extraction\n",
            "A Probabilistic Generative Model for Typographical Analysis of Early Modern Printing\n",
            "Attentive Pooling with Learnable Norms for Text Representation\n",
            "Estimating the influence of auxiliary tasks for multi-task learning of sequence tagging tasks\n",
            "How Does Selective Mechanism Improve Self-Attention Networks?\n",
            "Improving Transformer Models by Reordering their Sublayers\n",
            "Single Model Ensemble using Pseudo-Tags and Distinct Vectors\n",
            "Zero-shot Text Classification via Reinforced Self-training\n",
            "A Novel Graph-based Multi-modal Fusion Encoder for Neural Machine Translation\n",
            "A Relaxed Matching Procedure for Unsupervised BLI\n",
            "Dynamic Programming Encoding for Subword Segmentation in Neural Machine Translation\n",
            "Geometry-aware domain adaptation for unsupervised alignment of word embeddings\n",
            "Learning to Recover from Multi-Modality Errors for Non-Autoregressive Neural Machine Translation\n",
            "On the Inference Calibration of Neural Machine Translation\n",
            "Camouflaged Chinese Spam Content Detection with Semi-supervised Generative Active Learning\n",
            "Distinguish Confusing Law Articles for Legal Judgment Prediction\n",
            "Hiring Now: A Skill-Aware Multi-Attention Model for Job Posting Generation\n",
            "HyperCore: Hyperbolic and Co-graph Representation for Automatic ICD Coding\n",
            "Hyperbolic Capsule Networks for Multi-Label Classification\n",
            "Improving Segmentation for Technical Support Problems\n",
            "MOOCCube: A Large-scale Data Repository for NLP Applications in MOOCs\n",
            "Towards Interpretable Clinical Diagnosis with Bayesian Network Ensembles Stacked on Entity-Aware CNNs\n",
            "Analyzing the Persuasive Effect of Style in News Editorial Argumentation\n",
            "ECPE-2D: Emotion-Cause Pair Extraction based on Joint Two-Dimensional Representation, Interaction and Prediction\n",
            "Effective Inter-Clause Modeling for End-to-End Emotion-Cause Pair Extraction\n",
            "Embarrassingly Simple Unsupervised Aspect Extraction\n",
            "Enhancing Cross-target Stance Detection with Transferable Semantic-Emotion Knowledge\n",
            "KinGDOM: Knowledge-Guided DOMain Adaptation for Sentiment Analysis\n",
            "Modelling Context and Syntactical Features for Aspect-based Sentiment Analysis\n",
            "Parallel Data Augmentation for Formality Style Transfer\n",
            "Relational Graph Attention Network for Aspect-based Sentiment Analysis\n",
            "SpanMlt: A Span-based Multi-Task Learning Framework for Pair-wise Aspect and Opinion Terms Extraction\n",
            "Syntax-Aware Opinion Role Labeling with Dependency Graph Convolutional Networks\n",
            "Towards Better Non-Tree Argument Mining: Proposition-Level Biaffine Parsing with Task-Specific Parameterization\n",
            "A Span-based Linearization for Constituent Trees\n",
            "An Empirical Comparison of Unsupervised Constituency Parsing Methods\n",
            "Efficient Constituency Parsing by Pointing\n",
            "Efficient Second-Order TreeCRF for Neural Dependency Parsing\n",
            "Representations of Syntax [MASK] Useful: Effects of Constituency and Dependency Structure in Recursive LSTMs\n",
            "Structure-Level Knowledge Distillation For Multilingual Sequence Labeling\n",
            "Dynamic Online Conversation Recommendation\n",
            "Improving Multimodal Named Entity Recognition via Entity Span Detection with Unified Multimodal Transformer\n",
            "Stock Embeddings Acquired from News Articles and Price History, and an Application to Portfolio Optimization\n",
            "What Was Written vs. Who Read It: News Media Profiling Using Text Analysis and Social Media Context\n",
            "An Analysis of the Utility of Explicit Negative Examples to Improve the Syntactic Abilities of Neural Language Models\n",
            "On the Robustness of Language Encoders against Grammatical Errors\n",
            "Roles and Utilization of Attention Heads in Transformer-based Neural Language Models\n",
            "Understanding Attention for Text Classification\n",
            "A Relational Memory-based Embedding Model for Triple Classification and Search Personalization\n",
            "Do you have the right scissors? Tailoring Pre-trained Language Models via Monte-Carlo Methods\n",
            "Enhancing Pre-trained Chinese Character Representation with Word-aligned Attention\n",
            "On the Encoder-Decoder Incompatibility in Variational Text Modeling and Beyond\n",
            "SAFER: A Structure-free Approach for Certified Robustness to Adversarial Word Substitutions\n",
            "A Graph-based Coarse-to-fine Method for Unsupervised Bilingual Lexicon Induction\n",
            "A Reinforced Generation of Adversarial Examples for Neural Machine Translation\n",
            "A Retrieve-and-Rewrite Initialization Method for Unsupervised Machine Translation\n",
            "A Simple and Effective Unified Encoder for Document-Level Machine Translation\n",
            "Does Multi-Encoder Help? A Case Study on Context-Aware Neural Machine Translation\n",
            "Dynamically Adjusting Transformer Batch Size by Monitoring Gradient Direction Change\n",
            "Knowledge Distillation for Multilingual Unsupervised Neural Machine Translation\n",
            "Lexically Constrained Neural Machine Translation with Levenshtein Transformer\n",
            "On Exposure Bias, Hallucination and Domain Shift in Neural Machine Translation\n",
            "Automatic Machine Translation Evaluation using Source Language Inputs and Cross-lingual Language Model\n",
            "ChartDialogs: Plotting from Natural Language Instructions\n",
            "GLUECoS: An Evaluation Benchmark for Code-Switched NLP\n",
            "MATINF: A Jointly Labeled Large-Scale Dataset for Classification, Question Answering and Summarization\n",
            "MIND: A Large-scale Dataset for News Recommendation\n",
            "That is a Known Lie: Detecting Previously Fact-Checked Claims\n",
            "Towards Holistic and Automatic Evaluation of Open-Domain Dialogue Generation\n",
            "BiRRE: Learning Bidirectional Residual Relation Embeddings for Supervised Hypernymy Detection\n",
            "Biomedical Entity Representations with Synonym Marginalization\n",
            "Hypernymy Detection for Low-Resource Languages via Meta Learning\n",
            "Investigating Word-Class Distributions in Word Vector Spaces\n",
            "Aspect Sentiment Classification with Document-level Sentiment Preference Modeling\n",
            "Don’t Eclipse Your Arts Due to Small Discrepancies: Boundary Repositioning with a Pointer Network for Aspect Extraction\n",
            "Relation-Aware Collaborative Learning for Unified Aspect-Based Sentiment Analysis\n",
            "SentiBERT: A Transferable Transformer-Based Architecture for Compositional Sentiment Semantics\n",
            "Transition-based Directed Graph Construction for Emotion-Cause Pair Extraction\n",
            "CH-SIMS: A Chinese Multimodal Sentiment Analysis Dataset with Fine-grained Annotation of Modality\n",
            "Curriculum Pre-training for End-to-End Speech Translation\n",
            "How Accents Confound: Probing for Accent Information in End-to-End Speech Recognition Systems\n",
            "Improving Disfluency Detection by Self-Training a Self-Attentive Model\n",
            "Learning Spoken Language Representations with Neural Lattice Language Modeling\n",
            "Meta-Transfer Learning for Code-Switched Speech Recognition\n",
            "Reasoning with Multimodal Sarcastic Tweets via Modeling Cross-Modality Contrast and Semantic Association\n",
            "SimulSpeech: End-to-End Simultaneous Speech to Text Translation\n",
            "Towards end-2-end learning for predicting behavior codes from spoken utterances in psychotherapy conversations\n",
            "Neural Temporal Opinion Modelling for Opinion Prediction on Twitter\n",
            "It Takes Two to Lie: One to Lie, and One to Listen\n",
            "Learning Implicit Text Generation via Feature Matching\n",
            "Two Birds, One Stone: A Simple, Unified Model for Text Generation from Structured and Unstructured Data\n",
            "Bayesian Hierarchical Words Representation Learning\n",
            "Pre-training Is (Almost) All You Need: An Application to Commonsense Reasoning\n",
            "SEEK: Segmented Embedding of Knowledge Graphs\n",
            "Selecting Backtranslated Data from Multiple Sources for Improved Neural Machine Translation\n",
            "Successfully Applying the Stabilized Lottery Ticket Hypothesis to the Transformer Architecture\n",
            "A Self-Training Method for Machine Reading Comprehension with Soft Evidence Extraction\n",
            "Graph-to-Tree Learning for Solving Math Word Problems\n",
            "An Effectiveness Metric for Ordinal Classification: Formal Properties and Experimental Results\n",
            "Adaptive Compression of Word Embeddings\n",
            "Analysing Lexical Semantic Change with Contextualised Word Representations\n",
            "Autoencoding Keyword Correlation Graph for Document Clustering\n",
            "Autoencoding Pixies: Amortised Variational Inference with Graph Convolutions for Functional Distributional Semantics\n",
            "BERTRAM: Improved Word Embeddings Have Big Impact on Contextualized Model Performance\n",
            "CluBERT: A Cluster-Based Approach for Learning Sense Distributions in Multiple Languages\n",
            "Adversarial and Domain-Aware BERT for Cross-Domain Sentiment Analysis\n",
            "From Arguments to Key Points: Towards Automatic Argument Summarization\n",
            "GoEmotions: A Dataset of Fine-Grained Emotions\n",
            "He said “who’s gonna take care of your children when you are at ACL?”: Reported Sexist Acts are Not Sexist\n",
            "SKEP: Sentiment Knowledge Enhanced Pre-training for Sentiment Analysis\n",
            "Do Neural Language Models Show Preferences for Syntactic Formalisms?\n",
            "Enriched In-Order Linearization for Faster Sequence-to-Sequence Constituent Parsing\n",
            "Exact yet Efficient Graph Parsing, Bi-directional Locality and the Constructivist Hypothesis\n",
            "Max-Margin Incremental CCG Parsing\n",
            "Neural Reranking for Dependency Parsing: An Evaluation\n",
            "Demographics Should Not Be the Reason of Toxicity: Mitigating Discrimination in Text Classifications with Instance Weighting\n",
            "Analyzing analytical methods: The case of phonology in neural models of spoken language\n",
            "Make Up Your Mind! Adversarial Generation of Inconsistent Natural Language Explanations\n",
            "Perturbed Masking: Parameter-free Probing for Analyzing and Interpreting BERT\n",
            "Probing for Referential Information in Language Models\n",
            "Quantifying Attention Flow in Transformers\n",
            "Towards Faithfully Interpretable NLP Systems: How Should We Define and Evaluate Faithfulness?\n",
            "Towards Transparent and Explainable Attention Models\n",
            "Tchebycheff Procedure for Multi-task Text Classification\n",
            "Modeling Word Formation in English–German Neural Machine Translation\n",
            "Empowering Active Learning to Jointly Optimize System and User Demands\n",
            "Encoder-Decoder Models Can Benefit from Pre-trained Masked Language Models in Grammatical Error Correction\n",
            "Graph Neural News Recommendation with Unsupervised Preference Disentanglement\n",
            "Identifying Principals and Accessories in a Complex Case based on the Comprehension of Fact Description\n",
            "Joint Modelling of Emotion and Abusive Language Detection\n",
            "Programming in Natural Language with fuSE: Synthesizing Methods from Spoken Utterances Using Deep Natural Language Understanding\n",
            "Toxicity Detection: Does Context Really Matter?\n",
            "AMR Parsing with Latent Structural Information\n",
            "TaPas: Weakly Supervised Table Parsing via Pre-training\n",
            "Target Inference in Argument Conclusion Generation\n",
            "Multimodal Transformer for Multimodal Machine Translation\n",
            "Sentiment and Emotion help Sarcasm? A Multi-task Learning Framework for Multi-Modal Sarcasm, Sentiment and Emotion Analysis\n",
            "Towards Emotion-aided Multi-modal Dialogue Act Classification\n",
            "Analyzing Political Parody in Social Media\n",
            "Masking Actor Information Leads to Fairer Political Claims Detection\n",
            "When do Word Embeddings Accurately Reflect Surveys on our Beliefs About People?\n",
            "“Who said it, and Why?” Provenance for Natural Language Claims\n",
            "Compositionality and Generalization In Emergent Languages\n",
            "ERASER: A Benchmark to Evaluate Rationalized NLP Models\n",
            "Learning to Faithfully Rationalize by Construction\n",
            "Clinical Reading Comprehension: A Thorough Analysis of the emrQA Dataset\n",
            "DeFormer: Decomposing Pre-trained Transformers for Faster Question Answering\n",
            "Improving Multi-hop Question Answering over Knowledge Graphs using Knowledge Base Embeddings\n",
            "Template-Based Question Generation from Retrieved Sentences for Improved Unsupervised Question Answering\n",
            "Unsupervised Alignment-based Iterative Evidence Retrieval for Multi-hop Question Answering\n",
            "A Corpus for Large-Scale Phonetic Typology\n",
            "Dscorer: A Fast Evaluation Metric for Discourse Representation Structure Parsing\n",
            "ParaCrawl: Web-Scale Acquisition of Parallel Corpora\n",
            "Toward Gender-Inclusive Coreference Resolution\n",
            "Human Attention Maps for Text Classification: Do Humans and Neural Networks Focus on the Same Words?\n",
            "Information-Theoretic Probing for Linguistic Structure\n",
            "On the Cross-lingual Transferability of Monolingual Representations\n",
            "Similarity Analysis of Contextual Word Representation Models\n",
            "SenseBERT: Driving Some Sense into BERT\n",
            "ASSET: A Dataset for Tuning and Evaluation of Sentence Simplification Models with Multiple Rewriting Transformations\n",
            "Fatality Killed the Cat or: BabelPic, a Multimodal Dataset for Non-Concrete Concepts\n",
            "Modeling Label Semantics for Predicting Emotional Reactions\n",
            "CraftAssist Instruction Parsing: Semantic Parsing for a Voxel-World Assistant\n",
            "Don’t Say That! Making Inconsistent Dialogue Unlikely with Unlikelihood Training\n",
            "How does BERT’s attention change when you fine-tune? An analysis methodology and a case study in negation scope\n",
            "Influence Paths for Characterizing Subject-Verb Number Agreement in LSTM Language Models\n",
            "Interpreting Pretrained Contextualized Representations via Reductions to Static Embeddings\n",
            "Learning to Deceive with Attention-Based Explanations\n",
            "On the Spontaneous Emergence of Discrete and Compositional Signals\n",
            "Spying on Your Neighbors: Fine-grained Probing of Contextual Embeddings for Information about Surrounding Words\n",
            "Dense-Caption Matching and Frame-Selection Gating for Temporal Localization in VideoQA\n",
            "Shaping Visual Representations with Language for Few-Shot Classification\n",
            "Discrete Latent Variable Representations for Low-Resource Text Classification\n",
            "Learning Constraints for Structured Prediction Using Rectifier Networks\n",
            "Pretraining with Contrastive Sentence Objectives Improves Discourse Performance of Language Models\n",
            "A Recipe for Creating Multimodal Aligned Datasets for Sequential Tasks\n",
            "Adversarial NLI: A New Benchmark for Natural Language Understanding\n",
            "Beyond Accuracy: Behavioral Testing of NLP Models with CheckList\n",
            "Code and Named Entity Recognition in StackOverflow\n",
            "Dialogue-Based Relation Extraction\n",
            "Facet-Aware Evaluation for Extractive Summarization\n",
            "More Diverse Dialogue Datasets via Diversity-Informed Data Collection\n",
            "S2ORC: The Semantic Scholar Open Research Corpus\n",
            "Tangled up in BLEU: Reevaluating the Evaluation of Automatic Machine Translation Evaluation Metrics\n",
            "A Transformer-based Approach for Source Code Summarization\n",
            "Asking and Answering Questions to Evaluate the Factual Consistency of Summaries\n",
            "Discourse-Aware Neural Extractive Text Summarization\n",
            "Discrete Optimization for Unsupervised Sentence Summarization with Word-Level Extraction\n",
            "Exploring Content Selection in Summarization of Novel Chapters\n",
            "FEQA: A Question Answering Evaluation Framework for Faithfulness Assessment in Abstractive Summarization\n",
            "Fact-based Content Weighting for Evaluating Abstractive Summarisation\n",
            "Hooks in the Headline: Learning to Generate Headlines with Controlled Styles\n",
            "Knowledge Graph-Augmented Abstractive Summarization with Semantic-Driven Cloze Reward\n",
            "Optimizing the Factual Correctness of a Summary: A Study of Summarizing Radiology Reports\n",
            "Storytelling with Dialogue: A Critical Role Dungeons and Dragons Dataset\n",
            "The Summary Loop: Learning to Write Abstractive Summaries Without Examples\n",
            "Unsupervised Opinion Summarization as Copycat-Review Generation\n",
            "(Re)construing Meaning in NLP\n",
            "Climbing towards NLU: On Meaning, Form, and Understanding in the Age of Data\n",
            "Examining Citations of Natural Language Processing Literature\n",
            "How Can We Accelerate Progress Towards Human-like Linguistic Generalization?\n",
            "How Does NLP Benefit Legal System: A Summary of Legal Artificial Intelligence\n",
            "Intermediate-Task Transfer Learning with Pretrained Language Models: When and Why Does It Work?\n",
            "Predictive Biases in Natural Language Processing Models: A Conceptual Framework and Overview\n",
            "What Does BERT with Vision Look At?\n",
            "Balancing Objectives in Counseling Conversations: Advancing Forwards or Looking Backwards\n",
            "Detecting Perceived Emotions in Hurricane Disasters\n",
            "Hierarchical Modeling for User Personality Prediction: The Role of Message-Level Attention\n",
            "Measuring Forecasting Skill from Text\n",
            "Text and Causal Inference: A Review of Using Text to Remove Confounding from Causal Estimates\n",
            "Text-Based Ideal Points\n",
            "Understanding the Language of Political Agreement and Disagreement in Legislative Texts\n",
            "Would you Rather? A New Benchmark for Learning Machine Alignment with Cultural Values and Social Preferences\n",
            "Discourse as a Function of Event: Profiling Discourse Structure in News Articles around the Main Event\n",
            "Harnessing the linguistic signal to predict scalar inferences\n",
            "Implicit Discourse Relation Classification: We Need to Talk about Evaluation\n",
            "PeTra: A Sparsely Supervised Memory Model for People Tracking\n",
            "ZPR2: Joint Zero Pronoun Recovery and Resolution using Multi-Task Learning and BERT\n",
            "Contextualizing Hate Speech Classifiers with Post-hoc Explanation\n",
            "Double-Hard Debias: Tailoring Word Embeddings for Gender Bias Mitigation\n",
            "Language (Technology) is Power: A Critical Survey of “Bias” in NLP\n",
            "Social Bias Frames: Reasoning about Social and Power Implications of Language\n",
            "Social Biases in NLP Models as Barriers for Persons with Disabilities\n",
            "Towards Debiasing Sentence Representations\n",
            "A Re-evaluation of Knowledge Graph Completion Methods\n",
            "Cross-Linguistic Syntactic Evaluation of Word Prediction Models\n",
            "Evaluating Explainable AI: Which Algorithmic Explanations Help Users Predict Model Behavior?\n",
            "Explaining Black Box Predictions and Unveiling Data Artifacts through Influence Functions\n",
            "Finding Universal Grammatical Relations in Multilingual BERT\n",
            "Generating Hierarchical Explanations on Text Classification via Feature Interaction Detection\n",
            "Obtaining Faithful Interpretations from Compositional Neural Networks\n",
            "Rationalizing Text Matching: Learning Sparse Alignments via Optimal Transport\n",
            "Benefits of Intermediate Annotations in Reading Comprehension\n",
            "Crossing Variational Autoencoders for Answer Retrieval\n",
            "Logic-Guided Data Augmentation and Regularization for Consistent Question Answering\n",
            "On the Importance of Diversity in Question Generation for QA\n",
            "Probabilistic Assumptions Matter: Improved Models for Distantly-Supervised Document-Level Question Answering\n",
            "SCDE: Sentence Cloze Dataset with High Quality Distractors From Examinations\n",
            "Selective Question Answering under Domain Shift\n",
            "The Cascade Transformer: an Application for Efficient Answer Sentence Selection\n",
            "Transformers to Learn Hierarchical Contexts in Multiparty Dialogue for Span-based Question Answering\n",
            "Not All Claims are Created Equal: Choosing the Right Statistical Approach to Assess Hypotheses\n",
            "STARC: Structured Annotations for Reading Comprehension\n",
            "WinoWhy: A Deep Diagnosis of Essential Commonsense Knowledge for Answering Winograd Schema Challenge\n",
            "Agreement Prediction of Arguments in Cyber Argumentation for Detecting Stance Polarity and Intensity\n",
            "Cross-Lingual Unsupervised Sentiment Classification with Multi-View Transfer Learning\n",
            "Efficient Pairwise Annotation of Argument Quality\n",
            "Entity-Aware Dependency-Based Deep Graph Attention Network for Comparative Preference Classification\n",
            "OpinionDigest: A Simple Framework for Opinion Summarization\n",
            "A Comprehensive Analysis of Preprocessing for Word Representation Learning in Affective Tasks\n",
            "Diverse and Informative Dialogue Generation with Context-Specific Commonsense Knowledge Awareness\n",
            "Generate, Delete and Rewrite: A Three-Stage Framework for Improving Persona Consistency of Dialogue Generation\n",
            "Learning to Customize Model Structures for Few-shot Dialogue Generation Tasks\n",
            "Video-Grounded Dialogues with Pretrained Generation Language Models\n",
            "A Unified MRC Framework for Named Entity Recognition\n",
            "An Effective Transition-based Model for Discontinuous NER\n",
            "IMoJIE: Iterative Memory-Based Joint Open Information Extraction\n",
            "Improving Event Detection via Open-domain Trigger Knowledge\n",
            "Improving Low-Resource Named Entity Recognition using Joint Sentence and Token Labeling\n",
            "Multi-Cell Compositional LSTM for NER Domain Adaptation\n",
            "Pyramid: A Layered Model for Nested Named Entity Recognition\n",
            "ReInceptionE: Relation-Aware Inception Network with Joint Local-Global Structural Information for Knowledge Graph Embedding\n",
            "Relabel the Noise: Joint Extraction of Entities and Relations via Cooperative Multiagents\n",
            "Simplify the Usage of Lexicon in Chinese NER\n",
            "AdvAug: Robust Adversarial Augmentation for Neural Machine Translation\n",
            "Contextual Neural Machine Translation Improves Translation of Cataphoric Pronouns\n",
            "Improving Neural Machine Translation with Soft Template Prediction\n",
            "Tagged Back-translation Revisited: Why Does It Really Work?\n",
            "Worse WER, but Better BLEU? Leveraging Word Embedding as Intermediate in Multitask End-to-End Speech Translation\n",
            "Neural-DINF: A Neural Network based Framework for Measuring Document Influence\n",
            "Paraphrase Generation by Learning How to Edit from Samples\n",
            "Emerging Cross-lingual Structure in Pretrained Language Models\n",
            "FastBERT: a Self-distilling BERT with Adaptive Inference Time\n",
            "Incorporating External Knowledge through Pre-training for Natural Language to Code Generation\n",
            "LogicalFactChecker: Leveraging Logical Operations for Fact Checking with Graph Module Network\n",
            "Word-level Textual Adversarial Attacking as Combinatorial Optimization\n",
            "Benchmarking Multimodal Regex Synthesis with Complex Structures\n",
            "Curriculum Learning for Natural Language Understanding\n",
            "Do Neural Models Learn Systematicity of Monotonicity Inference in Natural Language?\n",
            "Evidence-Aware Inferential Text Generation with Vector Quantised Variational AutoEncoder\n",
            "How to Ask Good Questions? Try to Leverage Paraphrases\n",
            "NeuInfer: Knowledge Inference on N-ary Facts\n",
            "Neural Graph Matching Networks for Chinese Short Text Matching\n",
            "Neural Mixed Counting Models for Dispersed Topic Discovery\n",
            "Reasoning Over Semantic-Level Graph for Fact Checking\n",
            "Automatic Generation of Citation Texts in Scholarly Papers: A Pilot Study\n",
            "Composing Elementary Discourse Units in Abstractive Summarization\n",
            "Extractive Summarization as Text Matching\n",
            "Heterogeneous Graph Neural Networks for Extractive Document Summarization\n",
            "Jointly Learning to Align and Summarize for Neural Cross-Lingual Summarization\n",
            "Leveraging Graph to Improve Abstractive Multi-Document Summarization\n",
            "Multi-Granularity Interaction Network for Extractive and Abstractive Multi-Document Summarization\n",
            "Tetra-Tagging: Word-Synchronous Parsing with Linear-Time Inference\n",
            "Are we Estimating or Guesstimating Translation Quality?\n",
            "Language (Re)modelling: Towards Embodied Language Understanding\n",
            "The State and Fate of Linguistic Diversity and Inclusion in the NLP World\n",
            "The Unstoppable Rise of Computational Linguistics in Deep Learning\n",
            "To Boldly Query What No One Has Annotated Before? The Frontiers of Corpus Querying\n",
            "A Contextual Hierarchical Attention Network with Adaptive Objective for Dialogue State Tracking\n",
            "Data Manipulation: Towards Effective Instance Learning for Neural Dialogue Generation via Learning to Augment and Reweight\n",
            "Dynamic Fusion Network for Multi-Domain End-to-end Task-Oriented Dialog\n",
            "Learning Efficient Dialogue Policy from Demonstrations through Shaping\n",
            "SAS: Dialogue State Tracking via Slot Attention and Slot Information Sharing\n",
            "Speaker Sensitive Response Evaluation Model\n",
            "A Top-down Neural Architecture towards Text-level Parsing of Discourse Rhetorical Structure\n",
            "Amalgamation of protein sequence, structure and textual information for improving protein-protein interaction identification\n",
            "Bipartite Flat-Graph Network for Nested Named Entity Recognition\n",
            "Connecting Embeddings for Knowledge Graph Entity Typing\n",
            "Continual Relation Learning via Episodic Memory Activation and Reconsolidation\n",
            "Handling Rare Entities for Neural Sequence Labeling\n",
            "Instance-Based Learning of Span Representations: A Case Study through Named Entity Recognition\n",
            "MIE: A Medical Information Extractor towards Medical Dialogues\n",
            "Named Entity Recognition as Dependency Parsing\n",
            "Neighborhood Matching Network for Entity Alignment\n",
            "Relation Extraction with Explanation\n",
            "Representation Learning for Information Extraction from Form-like Documents\n",
            "Single-/Multi-Source Cross-Lingual NER via Teacher-Student Learning on Unlabeled Data in Target Language\n",
            "Synchronous Double-channel Recurrent Network for Aspect-Opinion Pair Extraction\n",
            "Cross-modal Coherence Modeling for Caption Generation\n",
            "Knowledge Supports Visual Language Grounding: A Case Study on Colour Terms\n",
            "Span-based Localizing Network for Natural Language Video Localization\n",
            "Words Aren’t Enough, Their Order Matters: On the Robustness of Grounding Visual Referring Expressions\n",
            "A Mixture of h - 1 Heads is Better than h Heads\n",
            "Dependency Graph Enhanced Dual-transformer Structure for Aspect-based Sentiment Classification\n",
            "Differentiable Window for Dynamic Local Attention\n",
            "Evaluating and Enhancing the Robustness of Neural Network-based Dependency Parsing Models with Adversarial Examples\n",
            "Exploiting Syntactic Structure for Better Language Modeling: A Syntactic Distance Approach\n",
            "Learning Architectures from an Extended Search Space for Language Modeling\n",
            "The Right Tool for the Job: Matching Model and Instance Complexities\n",
            "Bootstrapping Techniques for Polysynthetic Morphological Analysis\n",
            "Coupling Distant Annotation and Adversarial Training for Cross-Domain Chinese Word Segmentation\n",
            "Modeling Morphological Typology for Unsupervised Learning of Language Morphology\n",
            "Predicting Declension Class from Form and Meaning\n",
            "Unsupervised Morphological Paradigm Completion\n",
            "Document Modeling with Graph Attention Networks for Multi-grained Machine Reading Comprehension\n",
            "Harvesting and Refining Question-Answer Pairs for Unsupervised QA\n",
            "Low-Resource Generation of Multi-hop Reasoning Questions\n",
            "R4C: A Benchmark for Evaluating RC Systems to Get the Right Answer for the Right Reason\n",
            "Recurrent Chunking Mechanisms for Long-Text Machine Reading Comprehension\n",
            "RikiNet: Reading Wikipedia Pages for Natural Question Answering\n",
            "Parsing into Variable-in-situ Logico-Semantic Graphs\n",
            "Semantic Parsing for English as a Second Language\n",
            "Semi-Supervised Semantic Dependency Parsing Using CRF Autoencoders\n",
            "Unsupervised Dual Paraphrasing for Two-stage Semantic Parsing\n",
            "DRTS Parsing with Structure-Aware Encoding and Decoding\n",
            "A Two-Stage Masked LM Method for Term Set Expansion\n",
            "FLAT: Chinese NER Using Flat-Lattice Transformer\n",
            "Improving Entity Linking through Semantic Reinforced Entity Embeddings\n",
            "Document Translation vs. Query Translation for Cross-Lingual Information Retrieval in the Medical Domain\n",
            "Learning Robust Models for e-Commerce Product Search\n",
            "Generalized Entropy Regularization or: There’s Nothing Special about Label Smoothing\n",
            "Highway Transformer: Self-Gating Enhanced Self-Attentive Networks\n",
            "Low-Dimensional Hyperbolic Knowledge Graph Embeddings\n",
            "Classification-Based Self-Learning for Weakly Supervised Bilingual Lexicon Induction\n",
            "Gender in Danger? Evaluating Speech Translation Technology on the MuST-SHE Corpus\n",
            "Uncertainty-Aware Curriculum Learning for Neural Machine Translation\n",
            "Closing the Gap: Joint De-Identification and Concept Extraction in the Clinical Domain\n",
            "CorefQA: Coreference Resolution as Query-based Span Prediction\n",
            "Estimating predictive uncertainty for rumour verification models\n",
            "From Zero to Hero: Human-In-The-Loop Entity Linking in Low Resource Domains\n",
            "Language to Network: Conditional Parameter Adaptation with Natural Language Descriptions\n",
            "Controlled Crowdsourcing for High-Quality QA-SRL Annotation\n",
            "Cross-Lingual Semantic Role Labeling with High-Quality Translated Training Corpus\n",
            "Sentence Meta-Embeddings for Unsupervised Semantic Textual Similarity\n",
            "Transition-based Semantic Dependency Parsing with Pointer Networks\n",
            "tBERT: Topic Models and BERT Joining Forces for Semantic Similarity Detection\n",
            "Conditional Augmentation for Aspect Term Extraction via Masked Sequence-to-Sequence Generation\n",
            "Exploiting Personal Characteristics of Debaters for Predicting Persuasiveness\n",
            "Out of the Echo Chamber: Detecting Countering Debate Speeches\n",
            "Diversifying Dialogue Generation with Non-Conversational Text\n",
            "KdConv: A Chinese Multi-domain Dialogue Dataset Towards Multi-turn Knowledge-driven Conversation\n",
            "Meta-Reinforced Multi-Domain State Generator for Dialogue Systems\n",
            "Modeling Long Context for Task-Oriented Dialogue State Generation\n",
            "Multi-Domain Dialogue Acts and Response Co-Generation\n",
            "Exploring Contextual Word-level Style Relevance for Unsupervised Style Transfer\n",
            "Heterogeneous Graph Transformer for Graph-to-Sequence Learning\n",
            "Neural Data-to-Text Generation via Jointly Learning the Segmentation and Correspondence\n",
            "Aligned Dual Channel Graph Convolutional Network for Visual Question Answering\n",
            "Multimodal Neural Graph Memory Networks for Visual Question Answering\n",
            "Refer360∘: A Referring Expression Recognition Dataset in 360∘ Images\n",
            "CamemBERT: a Tasty French Language Model\n",
            "Effective Estimation of Deep Generative Language Models\n",
            "Null It Out: Guarding Protected Attributes by Iterative Nullspace Projection\n",
            "2kenize: Tying Subword Sequences for Chinese Script Conversion\n",
            "Predicting the Growth of Morphological Families from Social and Linguistic Factors\n",
            "Semi-supervised Contextual Historical Text Normalization\n",
            "ClarQ: A large-scale and diverse dataset for Clarification Question Generation\n",
            "DoQA - Accessing Domain-Specific FAQs via Conversational QA\n",
            "MLQA: Evaluating Cross-lingual Extractive Question Answering\n",
            "Multi-source Meta Transfer for Low Resource Multiple-Choice Question Answering\n",
            "Fine-grained Fact Verification with Kernel Graph Attention Network\n",
            "Generating Fact Checking Explanations\n",
            "Premise Selection in Natural Language Mathematical Texts\n",
            "A Call for More Rigor in Unsupervised Cross-lingual Learning\n",
            "A Tale of a Probe and a Parser\n",
            "From SPMRL to NMRL: What Did We Learn (and Unlearn) in a Decade of Parsing Morphologically-Rich Languages (MRLs)?\n",
            "Speech Translation and the End-to-End Promise: Taking Stock of Where We Are\n",
            "What Question Answering can Learn from Trivia Nerds\n",
            "What are the Goals of Distributional Semantics?\n",
            "Improving Image Captioning with Better Use of Caption\n",
            "Shape of Synth to Come: Why We Should Use Synthetic Data for English Surface Realization\n",
            "Toward Better Storylines with Sentence-Level Language Models\n",
            "A Two-Step Approach for Implicit Event Argument Detection\n",
            "Machine Reading of Historical Events\n",
            "Revisiting Unsupervised Relation Extraction\n",
            "SciREX: A Challenge Dataset for Document-Level Information Extraction\n",
            "Contrastive Self-Supervised Learning for Commonsense Reasoning\n",
            "Do Transformers Need Deep Long-Range Memory?\n",
            "Improving Disentangled Text Representation Learning with Information-Theoretic Guidance\n",
            "Understanding Advertisements with BERT\n",
            "Non-Linear Instance-Based Cross-Lingual Mapping for Non-Isomorphic Embedding Spaces\n",
            "Good-Enough Compositional Data Augmentation\n",
            "RAT-SQL: Relation-Aware Schema Encoding and Linking for Text-to-SQL Parsers\n",
            "Temporal Common Sense Acquisition with Minimal Supervision\n",
            "The Sensitivity of Language Models and Humans to Winograd Schema Perturbations\n",
            "Temporally-Informed Analysis of Named Entity Recognition\n",
            "Towards Open Domain Event Trigger Identification using Adversarial Domain Adaptation\n",
            "CompGuessWhat?!: A Multi-task Evaluation Framework for Grounded Language Learning\n",
            "Cross-Modality Relevance for Reasoning on Language and Vision\n",
            "Learning Web-based Procedures by Reasoning over Explanations and Demonstrations in Context\n",
            "Multi-agent Communication meets Natural Language: Synergies between Functional and Structural Language Learning\n",
            "HAT: Hardware-Aware Transformers for Efficient Natural Language Processing\n",
            "Hard-Coded Gaussian Attention for Neural Machine Translation\n",
            "In Neural Machine Translation, What Does Transfer Learning Transfer?\n",
            "Learning a Multi-Domain Curriculum for Neural Machine Translation\n",
            "Reducing Gender Bias in Neural Machine Translation as a Domain Adaptation Problem\n",
            "Translationese as a Language in “Multilingual” NMT\n",
            "Unsupervised Domain Clusters in Pretrained Language Models\n",
            "Using Context in Neural Machine Translation Training Objectives\n",
            "Variational Neural Machine Translation with Normalizing Flows\n",
            "The Paradigm Discovery Problem\n",
            "Supervised Grapheme-to-Phoneme Conversion of Orthographic Schwas in Hindi and Punjabi\n",
            "Automated Evaluation of Writing – 50 Years and Counting\n",
            "Negated and Misprimed Probes for Pretrained Language Models: Birds Can Talk, But Cannot Fly\n",
            "On Forgetting to Cite Older Papers: An Analysis of the ACL Anthology\n",
            "Returning the N to NLP: Towards Contextually Personalized Classification Models\n",
            "To Test Machine Comprehension, Start by Defining Comprehension\n",
            "Gender Gap in Natural Language Processing Research: Disparities in Authorship and Citations\n",
            "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension\n",
            "BLEURT: Learning Robust Metrics for Text Generation\n",
            "Distilling Knowledge Learned in BERT for Text Generation\n",
            "ESPRIT: Explaining Solutions to Physical Reasoning Tasks\n",
            "Iterative Edit-Based Unsupervised Sentence Simplification\n",
            "Logical Natural Language Generation from Open-Domain Tables\n",
            "Neural CRF Model for Sentence Alignment in Text Simplification\n",
            "One Size Does Not Fit All: Generating and Evaluating Variable Number of Keyphrases\n",
            "Rˆ3: Reverse, Retrieve, and Rank for Sarcasm Generation with Commonsense Knowledge\n",
            "Structural Information Preserving for Graph-to-Text Generation\n",
            "A Joint Neural Model for Information Extraction with Global Features\n",
            "Document-Level Event Role Filler Extraction using Multi-Granularity Contextualized Encoding\n",
            "Exploiting the Syntax-Model Consistency for Neural Relation Extraction\n",
            "From English to Code-Switching: Transfer Learning with Strong Morphological Clues\n",
            "Learning Interpretable Relationships between Entities, Relations and Concepts via Bayesian Structure Learning on Open Domain Facts\n",
            "Multi-Sentence Argument Linking\n",
            "Rationalizing Medical Relation Prediction from Corpus-level Statistics\n",
            "Sources of Transfer in Multilingual Named Entity Recognition\n",
            "ZeroShotCeres: Zero-Shot Relation Extraction from Semi-Structured Webpages\n",
            "Soft Gazetteers for Low-Resource Named Entity Recognition\n",
            "A Prioritization Model for Suicidality Risk Assessment\n",
            "CluHTM - Semantic Hierarchical Topic Modeling based on CluWords\n",
            "Empower Entity Set Expansion via Language Model Probing\n",
            "Feature Projection for Improved Text Classification\n",
            "A negative case analysis of visual grounding methods for VQA\n",
            "History for Visual Dialog: Do we really need it?\n",
            "Mapping Natural Language Instructions to Mobile UI Action Sequences\n",
            "TVQA+: Spatio-Temporal Grounding for Video Question Answering\n",
            "Unsupervised Multimodal Neural Machine Translation with Pseudo Visual Pivoting\n",
            "A Multitask Learning Approach for Diacritic Restoration\n",
            "Frugal Paradigm Completion\n",
            "Improving Chinese Word Segmentation with Wordhood Memory Networks\n",
            "Joint Chinese Word Segmentation and Part-of-speech Tagging via Two-way Attentions of Auto-analyzed Knowledge\n",
            "Joint Diacritization, Lemmatization, Normalization, and Fine-Grained Morphological Tagging\n",
            "Phonetic and Visual Priors for Decipherment of Informal Romanization\n",
            "Active Learning for Coreference Resolution using Discrete Annotation\n",
            "Beyond Possession Existence: Duration and Co-Possession\n",
            "Don’t Stop Pretraining: Adapt Language Models to Domains and Tasks\n",
            "Estimating Mutual Information Between Dense Word Embeddings\n",
            "Exploring Unexplored Generalization Challenges for Cross-Database Semantic Parsing\n",
            "Predicting the Focus of Negation: Model and Error Analysis\n",
            "Structured Tuning for Semantic Role Labeling\n",
            "TaBERT: Pretraining for Joint Understanding of Textual and Tabular Data\n",
            "Universal Decompositional Semantic Parsing\n",
            "Unsupervised Cross-lingual Representation Learning at Scale\n",
            "A Generate-and-Rank Framework with Semantic Type Regularization for Biomedical Concept Normalization\n",
            "Hierarchical Entity Typing via Multi-level Learning to Rank\n",
            "Multi-Domain Named Entity Recognition with Genre-Aware and Agnostic Inference\n",
            "TXtract: Taxonomy-Aware Knowledge Extraction for Thousands of Product Categories\n",
            "TriggerNER: Learning with Entity Triggers as Explanations for Named Entity Recognition\n",
            "Addressing Posterior Collapse with Mutual Information for Improved Variational Neural Machine Translation\n",
            "Balancing Training for Multilingual Neural Machine Translation\n",
            "Evaluating Robustness to Input Perturbations for Neural Machine Translation\n",
            "Parallel Corpus Filtering via Pre-trained Language Models\n",
            "Regularized Context Gates on Transformer for Machine Translation\n",
            "A Multi-Perspective Architecture for Semantic Code Search\n",
            "Automated Topical Component Extraction Using Neural Network Attention Scores from Source-based Essay Scoring\n",
            "Clinical Concept Linking with Contextualized Neural Representations\n",
            "DeSePtion: Dual Sequence Prediction and Adversarial Examples for Improved Fact-Checking\n",
            "Let Me Choose: From Verbal Context to Font Selection\n",
            "Multi-Label and Multilingual News Framing Analysis\n",
            "Predicting Performance for Natural Language Processing Tasks\n",
            "ScriptWriter: Narrative-Guided Script Generation\n",
            "Should All Cross-Lingual Embeddings Speak English?\n",
            "Smart To-Do: Automatic Generation of To-Do Items from Emails\n",
            "Are Natural Language Inference Models IMPPRESsive? Learning IMPlicature and PRESupposition\n",
            "End-to-End Bias Mitigation by Modelling Biases in Corpora\n",
            "Mind the Trade-off: Debiasing NLU Models without Degrading the In-distribution Performance\n",
            "NILE : Natural Language Inference with Faithful Natural Language Explanations\n",
            "QuASE: Question-Answer Driven Sentence Encoding\n",
            "Towards Robustifying NLI Models Against Lexical Dataset Biases\n",
            "Uncertain Natural Language Inference\n",
            "Extracting Headless MWEs from Dependency Parse Trees: Parsing, Tagging, and Joint Modeling Approaches\n",
            "Revisiting Higher-Order Dependency Parsers\n",
            "SeqVAT: Virtual Adversarial Training for Semi-Supervised Sequence Labeling\n",
            "Treebank Embedding Vectors for Out-of-Domain Dependency Parsing\n",
            "Xiaomingbot: A Multilingual Robot News Reporter\n",
            "TextBrewer: An Open-Source Knowledge Distillation Toolkit for Natural Language Processing\n",
            "Syntactic Search by Example\n",
            "Tabouid: a Wikipedia-based word guessing game\n",
            "Talk to Papers: Bringing Neural Question Answering to Academic Search\n",
            "Personalized PageRank with Syntagmatic Information for Multilingual Word Sense Disambiguation\n",
            "pyBART: Evidence-based Syntactic Transformations for IE\n",
            "EVIDENCEMINER: Textual Evidence Discovery for Life Sciences\n",
            "Trialstreamer: Mapping and Browsing Medical Evidence in Real-Time\n",
            "SyntaxGym: An Online Platform for Targeted Evaluation of Language Models\n",
            "GAIA: A Fine-grained Multimedia Knowledge Extraction System\n",
            "Multilingual Universal Sentence Encoder for Semantic Retrieval\n",
            "BENTO: A Visual Platform for Building Clinical NLP Pipelines Based on CodaLab\n",
            "Stanza: A Python Natural Language Processing Toolkit for Many Human Languages\n",
            "jiant: A Software Toolkit for Research on General-Purpose Text Understanding Models\n",
            "The Microsoft Toolkit of Multi-Task Deep Neural Networks for Natural Language Understanding\n",
            "LinggleWrite: a Coaching System for Essay Writing\n",
            "CLIReval: Evaluating Machine Translation as a Cross-Lingual Information Retrieval Task\n",
            "ConvLab-2: An Open-Source Toolkit for Building, Evaluating, and Diagnosing Dialogue Systems\n",
            "OpusFilter: A Configurable Parallel Corpus Filtering Toolbox\n",
            "Label Noise in Context\n",
            "exBERT: A Visual Analysis Tool to Explore Learned Representations in Transformer Models\n",
            "Nakdan: Professional Hebrew Diacritizer\n",
            "Photon: A Robust Cross-Domain Text-to-SQL System\n",
            "Interactive Task Learning from GUI-Grounded Natural Language Instructions and Demonstrations\n",
            "MixingBoard: a Knowledgeable Stylized Integrated Text Generation Platform\n",
            "NLP Scholar: An Interactive Visual Explorer for Natural Language Processing Literature\n",
            "Stimulating Creativity with FunLines: A Case Study of Humor Generation in Headlines\n",
            "Usnea: An Authorship Tool for Interactive Fiction using Retrieval Based Semantic Parsing\n",
            "DIALOGPT : Large-Scale Generative Pre-training for Conversational Response Generation\n",
            "ADVISER: A Toolkit for Developing Multi-modal, Multi-domain and Socially-engaged Conversational Agents\n",
            "Prta: A System to Support the Analysis of Propaganda Techniques in the News\n",
            "Clinical-Coder: Assigning Interpretable ICD-10 Codes to Chinese Clinical Notes\n",
            "ESPnet-ST: All-in-One Speech Translation Toolkit\n",
            "Penman: An Open-Source Library and Tool for AMR Graphs\n",
            "Embedding-based Scientific Literature Discovery in a Text Editor Application\n",
            "MMPE: A Multi-Modal Interface using Handwriting, Touch Reordering, and Speech Commands for Post-Editing Machine Translation\n",
            "Torch-Struct: Deep Structured Prediction Library\n",
            "Conversation Learner - A Machine Teaching Tool for Building Dialog Managers for Task-Oriented Dialog Systems\n",
            "NSTM: Real-Time Query-Driven News Overview Composition at Bloomberg\n",
            "SUPP.AI: finding evidence for supplement-drug interactions\n",
            "LEAN-LIFE: A Label-Efficient Annotation Framework Towards Learning from Explanation\n",
            "What’s The Latest? A Question-driven News Chatbot\n",
            "Adaptive Transformers for Learning Multimodal Representations\n",
            "Story-level Text Style Transfer: A Proposal\n",
            "Unsupervised Paraphasia Classification in Aphasic Speech\n",
            "HGCN4MeSH: Hybrid Graph Convolution Network for MeSH Indexing\n",
            "Grammatical Error Correction Using Pseudo Learner Corpus Considering Learner’s Error Tendency\n",
            "Research on Task Discovery for Transfer Learning in Deep Neural Networks\n",
            "RPD: A Distance Function Between Word Embeddings\n",
            "Reflection-based Word Attribute Transfer\n",
            "Topic Balancing with Additive Regularization of Topic Models\n",
            "Combining Subword Representations into Word-level Representations in the Transformer Architecture\n",
            "Zero-shot North Korean to English Neural Machine Translation by Character Tokenization and Phoneme Decomposition\n",
            "Media Bias, the Social Sciences, and NLP: Automating Frame Analyses to Identify Bias by Word Choice and Labeling\n",
            "SCAR: Sentence Compression using Autoencoders for Reconstruction\n",
            "Feature Difference Makes Sense: A medical image captioning model exploiting feature difference and tag information\n",
            "Multi-Task Neural Model for Agglutinative Language Translation\n",
            "Considering Likelihood in NLP Classification Explanations with Occlusion and Language Modeling\n",
            "Non-Topical Coherence in Social Talk: A Call for Dialogue Model Enrichment\n",
            "Why is penguin more similar to polar bear than to sea gull? Analyzing conceptual knowledge in distributional models\n",
            "A Simple and Effective Dependency Parser for Telugu\n",
            "Pointwise Paraphrase Appraisal is Potentially Problematic\n",
            "Efficient Neural Machine Translation for Low-Resource Languages via Exploiting Related Languages\n",
            "Exploring Interpretability in Event Extraction: Multitask Learning of a Neural Event Classifier and an Explanation Decoder\n",
            "Crossing the Line: Where do Demographic Variables Fit into Humor Detection?\n",
            "Effectively Aligning and Filtering Parallel Corpora under Sparse Data Conditions\n",
            "Understanding Points of Correspondence between Sentences for Abstractive Summarization\n",
            "uBLEU: Uncertainty-Aware Automatic Evaluation Method for Open-Domain Dialogue Systems\n",
            "To compress or not to compress? A Finite-State approach to Nen verbal morphology\n",
            "AraDIC: Arabic Document Classification Using Image-Based Character Embeddings and Class-Balanced Loss\n",
            "Embeddings of Label Components for Sequence Labeling: A Case Study of Fine-grained Named Entity Recognition\n",
            "Building a Japanese Typo Dataset from Wikipedia’s Revision History\n",
            "Preventing Critical Scoring Errors in Short Answer Scoring with Confidence Estimation\n",
            "How much complexity does an RNN architecture need to learn syntax-sensitive dependencies?\n",
            "Unsupervised Multilingual Sentence Embeddings for Parallel Corpus Mining\n",
            "Logical Inferences with Comparatives and Generalized Quantifiers\n",
            "Enhancing Word Embeddings with Knowledge Extracted from Lexical Resources\n",
            "Pre-training via Leveraging Assisting Languages for Neural Machine Translation\n",
            "Checkpoint Reranking: An Approach to Select Better Hypothesis for Neural Machine Translation Systems\n",
            "Cross-Lingual Disaster-related Multi-label Tweet Classification with Manifold Mixup\n",
            "Inducing Grammar from Long Short-Term Memory Networks by Shapley Decomposition\n",
            "Exploring the Role of Context to Distinguish Rhetorical and Information-Seeking Questions\n",
            "Compositional Generalization by Factorizing Alignment and Translation\n",
            "#NotAWhore! A Computational Linguistic Perspective of Rape Culture and Victimization on Social Media\n",
            "Interpretability and Analysis in Neural NLP\n",
            "Integrating Ethics into the NLP Curriculum\n",
            "Achieving Common Ground in Multi-modal Dialogue\n",
            "Reviewing Natural Language Processing Research\n",
            "Stylized Text Generation: Approaches and Applications\n",
            "Multi-modal Information Extraction from Text, Semi-structured, and Tabular Data on the Web\n",
            "Commonsense Reasoning for Natural Language Processing\n",
            "Open-Domain Question Answering\n",
            "Extending ImageNet to Arabic using Arabic WordNet\n",
            "Toward General Scene Graph: Integration of Visual Semantic Knowledge with Entity Synset Alignment\n",
            "Visual Question Generation from Radiology Images\n",
            "On the role of effective and referring questions in GuessWhat?!\n",
            "Latent Alignment of Procedural Concepts in Multimodal Recipes\n",
            "Dynamic Sentence Boundary Detection for Simultaneous Translation\n",
            "End-to-End Speech Translation with Adversarial Training\n",
            "Robust Neural Machine Translation with ASR Errors\n",
            "Improving Autoregressive NMT with Non-Autoregressive Model\n",
            "Modeling Discourse Structure for Document-level Neural Machine Translation\n",
            "BIT’s system for the AutoSimTrans 2020\n",
            "Linguistic Features for Readability Assessment\n",
            "Using PRMSE to evaluate automated scoring systems in the presence of label noise\n",
            "Multiple Instance Learning for Content Feedback Localization without Annotation\n",
            "Complementary Systems for Off-Topic Spoken Response Detection\n",
            "CIMA: A Large Open Access Dialogue Dataset for Tutoring\n",
            "Becoming Linguistically Mature: Modeling English and German Children’s Writing Development Across School Grades\n",
            "Annotation and Classification of Evidence and Reasoning Revisions in Argumentative Writing\n",
            "Can Neural Networks Automatically Score Essay Traits?\n",
            "Tracking the Evolution of Written Language Competence in L2 Spanish Learners\n",
            "Distractor Analysis and Selection for Multiple-Choice Cloze Questions for Second-Language Learners\n",
            "Assisting Undergraduate Students in Writing Spanish Methodology Sections\n",
            "Applications of Natural Language Processing in Bilingual Language Teaching: An Indonesian-English Case Study\n",
            "An empirical investigation of neural methods for content scoring of science explanations\n",
            "An Exploratory Study of Argumentative Writing by Young Students: A transformer-based Approach\n",
            "Should You Fine-Tune BERT for Automated Essay Scoring?\n",
            "GECToR – Grammatical Error Correction: Tag, Not Rewrite\n",
            "Interpreting Neural CWI Classifiers’ Weights as Vocabulary Size\n",
            "Automated Scoring of Clinical Expressive Language Evaluation Tasks\n",
            "Context-based Automated Scoring of Complex Mathematical Responses\n",
            "Predicting the Difficulty and Response Time of Multiple Choice Questions Using Transfer Learning\n",
            "A Comparative Study of Synthetic Data Generation Methods for Grammatical Error Correction\n",
            "Quantifying 60 Years of Gender Bias in Biomedical Research with Word Embeddings\n",
            "Sequence-to-Set Semantic Tagging for Complex Query Reformulation and Automated Text Categorization in Biomedical IR using Self-Attention\n",
            "Interactive Extractive Search over Biomedical Corpora\n",
            "Improving Biomedical Analogical Retrieval with Embedding of Structural Dependencies\n",
            "DeSpin: a prototype system for detecting spin in biomedical publications\n",
            "Towards Visual Dialog for Radiology\n",
            "A BERT-based One-Pass Multi-Task Model for Clinical Temporal Relation Extraction\n",
            "Experimental Evaluation and Development of a Silver-Standard for the MIMIC-III Clinical Coding Dataset\n",
            "Comparative Analysis of Text Classification Approaches in Electronic Health Records\n",
            "Noise Pollution in Hospital Readmission Prediction: Long Document Classification with Reinforcement Learning\n",
            "Evaluating the Utility of Model Configurations and Data Augmentation on Clinical Semantic Textual Similarity\n",
            "Entity-Enriched Neural Models for Clinical Question Answering\n",
            "Evidence Inference 2.0: More Data, Better Models\n",
            "Personalized Early Stage Alzheimer’s Disease Detection: A Case Study of President Reagan’s Speeches\n",
            "BioMRC: A Dataset for Biomedical Machine Reading Comprehension\n",
            "Neural Transduction of Letter Position Dyslexia using an Anagram Matrix Representation\n",
            "Domain Adaptation and Instance Selection for Disease Syndrome Classification over Veterinary Clinical Notes\n",
            "Benchmark and Best Practices for Biomedical Knowledge Graph Embeddings\n",
            "Extensive Error Analysis and a Learning-Based Evaluation of Medical Entity Recognition Systems to Approximate User Experience\n",
            "A Data-driven Approach for Noise Reduction in Distantly Supervised Biomedical Relation Extraction\n",
            "Global Locality in Biomedical Relation and Event Extraction\n",
            "An Empirical Study of Multi-Task Learning on BERT for Biomedical Text Mining\n",
            "A Transformer-based joint-encoding for Emotion Recognition and Sentiment Analysis\n",
            "A Multi-modal Approach to Fine-grained Opinion Mining on Video Reviews\n",
            "Multilogue-Net: A Context-Aware RNN for Multi-modal Emotion Detection and Sentiment Analysis in Conversation\n",
            "Low Rank Fusion based Transformers for Multimodal Sequences\n",
            "Unsupervised Online Grounding of Natural Language during Human-Robot Interactions\n",
            "Leveraging Multimodal Behavioral Analytics for Automated Job Interview Performance Assessment and Feedback\n",
            "Audio-Visual Understanding of Passenger Intents for In-Cabin Conversational Agents\n",
            "AI Sensing for Robotics using Deep Learning based Visual and Language Modeling\n",
            "Exploring Weaknesses of VQA Models through Attribution Driven Insights\n",
            "Bootstrapping Named Entity Recognition in E-Commerce with Positive Unlabeled Learning\n",
            "How to Grow a (Product) Tree: Personalized Category Suggestions for eCommerce Type-Ahead\n",
            "Deep Learning-based Online Alternative Product Recommendations at Scale\n",
            "A Deep Learning System for Sentiment Analysis of Service Calls\n",
            "Using Large Pretrained Language Models for Answering User Queries from Product Specifications\n",
            "Improving Intent Classification in an E-commerce Voice Assistant by Using Inter-Utterance Context\n",
            "Semi-Supervised Iterative Approach for Domain-Specific Complaint Detection in Social Media\n",
            "Item-based Collaborative Filtering with BERT\n",
            "Semi-supervised Category-specific Review Tagging on Indonesian E-Commerce Product Reviews\n",
            "Deep Hierarchical Classification for Category Prediction in E-commerce System\n",
            "SimsterQ: A Similarity based Clustering Approach to Opinion Question Answering\n",
            "e-Commerce and Sentiment Analysis: Predicting Outcomes of Class Action Lawsuits\n",
            "On Application of Bayesian Parametric and Non-parametric Methods for User Cohorting in Product Search\n",
            "Simple Compounded-Label Training for Fact Extraction and Verification\n",
            "Stance Prediction and Claim Verification: An Arabic Perspective\n",
            "A Probabilistic Model with Commonsense Constraints for Pattern-based Temporal Fact Extraction\n",
            "Developing a How-to Tip Machine Comprehension Dataset and its Evaluation in Machine Comprehension by BERT\n",
            "Language Models as Fact Checkers?\n",
            "Maintaining Quality in FEVER Annotation\n",
            "Distilling the Evidence to Augment Fact Verification Models\n",
            "A Report on the 2020 Sarcasm Detection Shared Task\n",
            "Augmenting Data for Sarcasm Detection with Unlabeled Conversation Context\n",
            "A Report on the 2020 VUA and TOEFL Metaphor Detection Shared Task\n",
            "DeepMet: A Reading Comprehension Paradigm for Token-level Metaphor Detection\n",
            "Context-Driven Satirical News Generation\n",
            "Sarcasm Detection using Context Separators in Online Discourse\n",
            "Sarcasm Detection in Tweets with BERT and GloVe Embeddings\n",
            "C-Net: Contextual Network for Sarcasm Detection\n",
            "Applying Transformers and Aspect-based Sentiment Analysis approaches on Sarcasm Detection\n",
            "Sarcasm Identification and Detection in Conversion Context using BERT\n",
            "Neural Sarcasm Detection using Conversation Context\n",
            "Context-Aware Sarcasm Detection Using BERT\n",
            "Transformers on Sarcasm Detection with Context\n",
            "A Novel Hierarchical BERT Architecture for Sarcasm Detection\n",
            "Detecting Sarcasm in Conversation Context Using Transformer-Based Models\n",
            "Using Conceptual Norms for Metaphor Detection\n",
            "ALBERT-BiLSTM for Sequential Metaphor Detection\n",
            "Character aware models with similarity learning for metaphor detection\n",
            "Sky + Fire = Sunset. Exploring Parallels between Visually Grounded Metaphors and Image Classifiers\n",
            "Recognizing Euphemisms and Dysphemisms Using Sentiment Analysis\n",
            "IlliniMet: Illinois System for Metaphor Detection with Contextual and Linguistic Information\n",
            "Adaptation of Word-Level Benchmark Datasets for Relation-Level Metaphor Identification\n",
            "Generating Ethnographic Models from Communities’ Online Data\n",
            "Oxymorons: a preliminary corpus investigation\n",
            "Can Humor Prediction Datasets be used for Humor Generation? Humorous Headline Generation via Style Transfer\n",
            "Evaluating a Bi-LSTM Model for Metaphor Detection in TOEFL Essays\n",
            "Neural Metaphor Detection with a Residual biLSTM-CRF Model\n",
            "Augmenting Neural Metaphor Detection with Concreteness\n",
            "Supervised Disambiguation of German Verbal Idioms with a BiLSTM Architecture\n",
            "Metaphor Detection using Context and Concreteness\n",
            "Being neighbourly: Neural metaphor identification in discourse\n",
            "Go Figure! Multi-task transformer-based architecture for metaphor detection using idioms: ETS team in 2020 metaphor shared task\n",
            "Metaphor Detection using Ensembles of Bidirectional Recurrent Neural Networks\n",
            "Metaphor Detection Using Contextual Word Embeddings From Transformers\n",
            "Testing the role of metadata in metaphor identification\n",
            "Sarcasm Detection Using an Ensemble Approach\n",
            "A Transformer Approach to Contextual Sarcasm Detection in Twitter\n",
            "Transformer-based Context-aware Sarcasm Detection in Conversation Threads from Social Media\n",
            "Syntactic Parsing in Humans and Machines\n",
            "Distilling Neural Networks for Greener and Faster Dependency Parsing\n",
            "End-to-End Negation Resolution as Graph Parsing\n",
            "Integrating Graph-Based and Transition-Based Dependency Parsers in the Deep Contextualized Era\n",
            "Semi-supervised Parsing with a Variational Autoencoding Parser\n",
            "Memory-bounded Neural Incremental Parsing for Psycholinguistic Prediction\n",
            "Obfuscation for Privacy-preserving Syntactic Parsing\n",
            "Tensors over Semirings for Latent-Variable Weighted Logic Programs\n",
            "Advances in Using Grammars with Latent Annotations for Discontinuous Parsing\n",
            "Lexicalization of Probabilistic Linear Context-free Rewriting Systems\n",
            "Self-Training for Unsupervised Parsing with PRPN\n",
            "Span-Based LCFRS-2 Parsing\n",
            "Analysis of the Penn Korean Universal Dependency Treebank (PKT-UD): Manual Revision to Build Robust Parsing Model in Korean\n",
            "Statistical Deep Parsing for Spanish Using Neural Networks\n",
            "The Importance of Category Labels in Grammar Induction with Child-directed Utterances\n",
            "Overview of the IWPT 2020 Shared Task on Parsing into Enhanced Universal Dependencies\n",
            "Turku Enhanced Parser Pipeline: From Raw Text to Enhanced Graphs in the IWPT 2020 Shared Task\n",
            "Hybrid Enhanced Universal Dependencies Parsing\n",
            "Adaptation of Multilingual Transformer Encoder for Robust Enhanced Universal Dependency Parsing\n",
            "Efficient EUD Parsing\n",
            "Linear Neural Parsing and Hybrid Enhancement for Enhanced Universal Dependencies\n",
            "Enhanced Universal Dependency Parsing with Second-Order Inference and Mixture of Training Data\n",
            "How Much of Enhanced UD Is Contained in UD?\n",
            "The ADAPT Enhanced Dependency Parser at the IWPT 2020 Shared Task\n",
            "Køpsala: Transition-Based Graph Parsing via Efficient Training and Effective Encoding\n",
            "RobertNLP at the IWPT 2020 Shared Task: Surprisingly Simple Enhanced UD Parsing for English\n",
            "FINDINGS OF THE IWSLT 2020 EVALUATION CAMPAIGN\n",
            "ON-TRAC Consortium for End-to-End and Simultaneous Speech Translation Challenge Tasks at IWSLT 2020\n",
            "Start-Before-End and End-to-End: Neural Speech Translation by AppTek and RWTH Aachen University\n",
            "KIT’s IWSLT 2020 SLT Translation System\n",
            "End-to-End Simultaneous Translation System for IWSLT2020 Using Modality Agnostic Meta-Learning\n",
            "DiDi Labs’ End-to-end System for the IWSLT 2020 Offline Speech TranslationTask\n",
            "End-to-End Offline Speech Translation System for IWSLT 2020 using Modality Agnostic Meta-Learning\n",
            "End-to-End Speech-Translation with Knowledge Distillation: FBK@IWSLT2020\n",
            "SRPOL’s System for the IWSLT 2020 End-to-End Speech Translation Task\n",
            "The University of Helsinki Submission to the IWSLT2020 Offline SpeechTranslation Task\n",
            "The AFRL IWSLT 2020 Systems: Work-From-Home Edition\n",
            "LIT Team’s System Description for Japanese-Chinese Machine Translation Task in IWSLT 2020\n",
            "OPPO’s Machine Translation System for the IWSLT 2020 Open Domain Translation Task\n",
            "Character Mapping and Ad-hoc Adaptation: Edinburgh’s IWSLT 2020 Open Domain Translation System\n",
            "CASIA’s System for IWSLT 2020 Open Domain Translation\n",
            "Deep Blue Sonics’ Submission to IWSLT 2020 Open Domain Translation Task\n",
            "University of Tsukuba’s Machine Translation System for IWSLT20 Open Domain Translation Task\n",
            "Xiaomi’s Submissions for IWSLT 2020 Open Domain Translation Task\n",
            "ISTIC’s Neural Machine Translation System for IWSLT’2020\n",
            "Octanove Labs’ Japanese-Chinese Open Domain Translation System\n",
            "NAIST’s Machine Translation Systems for IWSLT 2020 Conversational Speech Translation Task\n",
            "Generating Fluent Translations from Disfluent Text Without Access to Fluent References: IIT Bombay@IWSLT2020\n",
            "The HW-TSC Video Speech Translation System at IWSLT 2020\n",
            "CUNI Neural ASR with Phoneme-Level Intermediate Step for~Non-Native~SLT at IWSLT 2020\n",
            "ELITR Non-Native Speech Translation at IWSLT 2020\n",
            "Is 42 the Answer to Everything in Subtitling-oriented Speech Translation?\n",
            "Re-translation versus Streaming for Simultaneous Translation\n",
            "Towards Stream Translation: Adaptive Computation Time for Simultaneous Machine Translation\n",
            "Neural Simultaneous Speech Translation Using Alignment-Based Chunking\n",
            "Adapting End-to-End Speech Recognition for Readable Subtitles\n",
            "From Speech-to-Speech Translation to Automatic Dubbing\n",
            "Joint Translation and Unit Conversion for End-to-end Localization\n",
            "Efficient Automatic Punctuation Restoration Using Bidirectional Transformers with Robust Inference\n",
            "How Human is Machine Translationese? Comparing Human and Machine Translations of Text and Speech\n",
            "Findings of the Fourth Workshop on Neural Generation and Translation\n",
            "Learning to Generate Multiple Style Transfer Outputs for an Input Sentence\n",
            "Balancing Cost and Benefit with Tied-Multi Transformers\n",
            "Compressing Neural Machine Translation Models with 4-bit Precision\n",
            "Meta-Learning for Few-Shot NMT Adaptation\n",
            "Automatically Ranked Russian Paraphrase Corpus for Text Generation\n",
            "A Deep Reinforced Model for Zero-Shot Cross-Lingual Summarization with Bilingual Semantic Similarity Rewards\n",
            "A Question Type Driven and Copy Loss Enhanced Frameworkfor Answer-Agnostic Neural Question Generation\n",
            "A Generative Approach to Titling and Clustering Wikipedia Sections\n",
            "The Unreasonable Volatility of Neural Machine Translation Models\n",
            "Leveraging Sentence Similarity in Natural Language Generation: Improving Beam Search using Range Voting\n",
            "Distill, Adapt, Distill: Training Small, In-Domain Models for Neural Machine Translation\n",
            "Training and Inference Methods for High-Coverage Neural Machine Translation\n",
            "Meeting the 2020 Duolingo Challenge on a Shoestring\n",
            "English-to-Japanese Diverse Translation by Combining Forward and Backward Outputs\n",
            "POSTECH Submission on Duolingo Shared Task\n",
            "The ADAPT System Description for the STAPLE 2020 English-to-Portuguese Translation Task\n",
            "Expand and Filter: CUNI and LMU Systems for the WNGT 2020 Duolingo Shared Task\n",
            "Exploring Model Consensus to Generate Translation Paraphrases\n",
            "Growing Together: Modeling Human Language Learning With n-Best Multi-Checkpoint Machine Translation\n",
            "Generating Diverse Translations via Weighted Fine-tuning and Hypotheses Filtering for the Duolingo STAPLE Task\n",
            "The JHU Submission to the 2020 Duolingo Shared Task on Simultaneous Translation and Paraphrase for Language Education\n",
            "Simultaneous paraphrasing and translation by fine-tuning Transformer models\n",
            "The NiuTrans System for WNGT 2020 Efficiency Task\n",
            "Efficient and High-Quality Neural Machine Translation with OpenNMT\n",
            "Edinburgh’s Submissions to the 2020 Machine Translation Efficiency Task\n",
            "Improving Document-Level Neural Machine Translation with Domain Adaptation\n",
            "Simultaneous Translation and Paraphrase for Language Education\n",
            "Answering Complex Questions by Combining Information from Curated and Extracted Knowledge Bases\n",
            "Towards Reversal-Based Textual Data Augmentation for NLI Problems with Opposable Classes\n",
            "Examination and Extension of Strategies for Improving Personalized Language Modeling via Interpolation\n",
            "Efficient Deployment of Conversational Natural Language Interfaces over Databases\n",
            "Neural Multi-task Text Normalization and Sanitization with Pointer-Generator\n",
            "Using Alternate Representations of Text for Natural Language Understanding\n",
            "On Incorporating Structural Information to improve Dialogue Response Generation\n",
            "CopyBERT: A Unified Approach to Question Generation with Self-Attention\n",
            "How to Tame Your Data: Data Augmentation for Dialog State Tracking\n",
            "Efficient Intent Detection with Dual Sentence Encoders\n",
            "Accelerating Natural Language Understanding in Task-Oriented Dialog\n",
            "DLGNet: A Transformer-based Model for Dialogue Response Generation\n",
            "Data Augmentation for Training Dialog Models Robust to Speech Recognition Errors\n",
            "Automating Template Creation for Ranking-Based Dialogue Models\n",
            "From Machine Reading Comprehension to Dialogue State Tracking: Bridging the Gap\n",
            "Improving Slot Filling by Utilizing Contextual Information\n",
            "Learning to Classify Intents and Slot Labels Given a Handful of Examples\n",
            "MultiWOZ 2.2 : A Dialogue Dataset with Additional Annotation Corrections and State Tracking Baselines\n",
            "Sketch-Fill-A-R: A Persona-Grounded Chit-Chat Generation Framework\n",
            "Probing Neural Dialog Models for Conversational Understanding\n",
            "CORD-19: The COVID-19 Open Research Dataset\n",
            "Rapidly Deploying a Neural Search Engine for the COVID-19 Open Research Dataset\n",
            "Document Classification for COVID-19 Literature\n",
            "Enabling Low-Resource Transfer Learning across COVID-19 Corpora by Combining Event-Extraction and Co-Training\n",
            "Self-supervised context-aware COVID-19 document exploration through atlas grounding\n",
            "CODA-19: Using a Non-Expert Crowd to Annotate Research Aspects on 10,000+ Abstracts in the COVID-19 Open Research Dataset\n",
            "Information Retrieval and Extraction on COVID-19 Clinical Articles Using Graph Community Detection and Bio-BERT Embeddings\n",
            "What Are People Asking About COVID-19? A Question Classification Dataset\n",
            "Jennifer for COVID-19: An NLP-Powered Chatbot Built for the People and by the People to Combat Misinformation\n",
            "A Natural Language Processing System for National COVID-19 Surveillance in the US Department of Veterans Affairs\n",
            "Measuring Emotions in the COVID-19 Real World Worry Dataset\n",
            "Estimating the effect of COVID-19 on mental health: Linguistic indicators of depression during a global pandemic\n",
            "Exploration of Gender Differences in COVID-19 Discourse on Reddit\n",
            "Cross-language sentiment analysis of European Twitter messages during the COVID-19 pandemic\n",
            "Cross-lingual Transfer Learning for COVID-19 Outbreak Alignment\n",
            "COVID-19 and Arabic Twitter: How can Arab World Governments and Public Health Organizations Learn from Social Media?\n",
            "NLP-based Feature Extraction for the Detection of COVID-19 Misinformation Videos on YouTube\n",
            "COVID-QA: A Question Answering Dataset for COVID-19\n",
            "Methods for Extracting Information from Messages from Primary Care Providers to Specialists\n",
            "Towards Understanding ASR Error Correction for Medical Conversations\n",
            "Studying Challenges in Medical Conversation with Structured Annotation\n",
            "Generating Medical Reports from Patient-Doctor Conversations Using Sequence-to-Sequence Models\n",
            "Towards an Ontology-based Medication Conversational Agent for PrEP and PEP\n",
            "Heart Failure Education of African American and Hispanic/Latino Patients: Data Collection and Analysis\n",
            "On the Utility of Audiovisual Dialog Technologies and Signal Analytics for Real-time Remote Monitoring of Depression Biomarkers\n",
            "Robust Prediction of Punctuation and Truecasing for Medical ASR\n",
            "Topic-Based Measures of Conversation for Detecting Mild CognitiveImpairment\n",
            "New Insights into Cross-Document Event Coreference: Systematic Comparison and a Simplified Approach\n",
            "Screenplay Quality Assessment: Can We Predict Who Gets Nominated?\n",
            "Improving the Identification of the Discourse Function of News Article Paragraphs\n",
            "Systematic Evaluation of a Framework for Unsupervised Emotion Recognition for Narrative Text\n",
            "Extensively Matching for Few-shot Learning Event Detection\n",
            "Exploring the Effect of Author and Reader Identity in Online Story Writing: the STORIESINTHEWILD Corpus.\n",
            "Script Induction as Association Rule Mining\n",
            "Automatic extraction of personal events from dialogue\n",
            "Annotating and quantifying narrative time disruptions in modernist and hypertext fiction\n",
            "Exploring aspects of similarity between spoken personal narratives by disentangling them into narrative clause types\n",
            "Extracting Message Sequence Charts from Hindi Narrative Text\n",
            "Emotion Arcs of Student Narratives\n",
            "Frustratingly Hard Evidence Retrieval for QA Over Books\n",
            "On-The-Fly Information Retrieval Augmentation for Language Models\n",
            "Detecting and understanding moral biases in news\n",
            "Zero-Resource Cross-Domain Named Entity Recognition\n",
            "Encodings of Source Syntax: Similarities in NMT Representations Across Target Languages\n",
            "Learning Probabilistic Sentence Representations from Paraphrases\n",
            "Word Embeddings as Tuples of Feature Probabilities\n",
            "Compositionality and Capacity in Emergent Languages\n",
            "Learning Geometric Word Meta-Embeddings\n",
            "Improving Bilingual Lexicon Induction with Unsupervised Post-Processing of Monolingual Word Vector Spaces\n",
            "Adversarial Training for Commonsense Inference\n",
            "Evaluating Natural Alpha Embeddings on Intrinsic and Extrinsic Tasks\n",
            "Exploring the Limits of Simple Learners in Knowledge Distillation for Document Classification with DocBERT\n",
            "Joint Training with Semantic Role Labeling for Better Generalization in Natural Language Inference\n",
            "A Metric Learning Approach to Misogyny Categorization\n",
            "On the Choice of Auxiliary Languages for Improved Sequence Tagging\n",
            "Adversarial Alignment of Multilingual Models for Extracting Temporal Expressions from Text\n",
            "Contextual and Non-Contextual Word Embeddings: an in-depth Linguistic Investigation\n",
            "Are All Languages Created Equal in Multilingual BERT?\n",
            "Staying True to Your Word: (How) Can Attention Become Explanation?\n",
            "Compressing BERT: Studying the Effects of Weight Pruning on Transfer Learning\n",
            "On Dimensional Linguistic Properties of the Word Embedding Space\n",
            "A Cross-Task Analysis of Text Span Representations\n",
            "Enhancing Transformer with Sememe Knowledge\n",
            "Evaluating Compositionality of Sentence Representation Models\n",
            "Supertagging with CCG primitives\n",
            "What’s in a Name? Are BERT Named Entity Representations just as Good for any other Name?\n",
            "SIGMORPHON 2020 Shared Task 0: Typologically Diverse Morphological Inflection\n",
            "The SIGMORPHON 2020 Shared Task on Multilingual Grapheme-to-Phoneme Conversion\n",
            "The SIGMORPHON 2020 Shared Task on Unsupervised Morphological Paradigm Completion\n",
            "One-Size-Fits-All Multilingual Models\n",
            "Ensemble Self-Training for Low-Resource Languages: Grapheme-to-Phoneme Conversion and Morphological Inflection\n",
            "The CMU-LTI submission to the SIGMORPHON 2020 Shared Task 0: Language-Specific Cross-Lingual Transfer\n",
            "Grapheme-to-Phoneme Conversion with a Multilingual Transformer Model\n",
            "The NYU-CUBoulder Systems for SIGMORPHON 2020 Task 0 and Task 2\n",
            "The IMS–CUBoulder System for the SIGMORPHON 2020 Shared Task on Unsupervised Morphological Paradigm Completion\n",
            "SIGMORPHON 2020 Task 0 System Description: ETH Zürich Team\n",
            "KU-CST at the SIGMORPHON 2020 Task 2 on Unsupervised Morphological Paradigm Completion\n",
            "Low-Resource G2P and P2G Conversion with Synthetic Training Data\n",
            "Frustratingly Easy Multilingual Grapheme-to-Phoneme Conversion\n",
            "Exploring Neural Architectures And Techniques For Typologically Diverse Morphological Inflection\n",
            "University of Illinois Submission to the SIGMORPHON 2020 Shared Task 0: Typologically Diverse Morphological Inflection\n",
            "One Model to Pronounce Them All: Multilingual Grapheme-to-Phoneme Conversion With a Transformer Ensemble\n",
            "Leveraging Principal Parts for Morphological Inflection\n",
            "Linguist vs. Machine: Rapid Development of Finite-State Morphological Grammars\n",
            "CLUZH at SIGMORPHON 2020 Shared Task on Multilingual Grapheme-to-Phoneme Conversion\n",
            "The UniMelb Submission to the SIGMORPHON 2020 Shared Task 0: Typologically Diverse Morphological Inflection\n",
            "Data Augmentation for Transformer-based G2P\n",
            "Transliteration for Cross-Lingual Morphological Inflection\n",
            "Evaluating Neural Morphological Taggers for Sanskrit\n",
            "Getting the ##life out of living: How Adequate Are Word-Pieces for Modelling Complex Morphology?\n",
            "Induced Inflection-Set Keyword Search in Speech\n",
            "Representation Learning for Discovering Phonemic Tone Contours\n",
            "Joint learning of constraint weights and gradient inputs in Gradient Symbolic Computation with constrained optimization\n",
            "In search of isoglosses: continuous and discrete language embeddings in Slavic historical phonology\n",
            "Multi-Tiered Strictly Local Functions\n",
            "Enhancing Bias Detection in Political News Using Pragmatic Presupposition\n",
            "Demoting Racial Bias in Hate Speech Detection\n",
            "NARMADA: Need and Available Resource Managing Assistant for Disasters and Adversities\n",
            "BEEP! Korean Corpus of Online News Comments for Toxic Speech Detection\n",
            "Stance Prediction for Contemporary Issues: Data and Experiments\n",
            "Challenges in Emotion Style Transfer: An Exploration with a Lexical Substitution Pipeline\n",
            "Incorporating Uncertain Segmentation Information into Chinese NER for Social Media Text\n",
            "Multi-Task Supervised Pretraining for Neural Domain Adaptation\n",
            "Corpus based Amharic sentiment lexicon generation\n",
            "Negation handling for Amharic sentiment classification\n",
            "Embedding Oriented Adaptable Semantic Annotation Framework for Amharic Web Documents\n",
            "Similarity and Farness Based Bidirectional Neural Co-Attention for Amharic Natural Language Inference\n",
            "Large Vocabulary Read Speech Corpora for Four Ethiopian Languages: Amharic, Tigrigna, Oromo, and Wolaytta\n",
            "SIMPLEX-PB 2.0: A Reliable Dataset for Lexical Simplification in Brazilian Portuguese\n",
            "Bi-directional Answer-to-Answer Co-attention for Short Answer Grading using Deep Learning\n",
            "Effective questions in referential visual dialogue\n",
            "A Translation-Based Approach to Morphology Learning for Low Resource Languages\n",
            "Tigrinya Automatic Speech recognition with Morpheme based recognition units\n",
            "Variants of Vector Space Reductions for Predicting the Compositionality of English Noun Compounds\n",
            "An Assessment of Language Identification Methods on Tweets and Wikipedia Articles\n",
            "A Comparison of Identification Methods of Brazilian Music Styles by Lyrics\n",
            "Enabling fast and correct typing in ‘Leichte Sprache’ (Easy Language)\n",
            "AI4D - African Language Dataset Challenge\n",
            "Can Wikipedia Categories Improve Masked Language Model Pretraining?\n",
            "FFR v1.1: Fon-French Neural Machine Translation\n",
            "Classification and Analysis of Neologisms Produced by Learners of Spanish: Effects of Proficiency and Task\n",
            "Developing a Monolingual Sentence Simplification Corpus for Urdu\n",
            "Translating Natural Language Instructions for Behavioral Robot Navigation with a Multi-Head Attention Mechanism\n",
            "Towards Mitigating Gender Bias in a decoder-based Neural Machine Translation model by Adding Contextual Information\n",
            "Predicting and Analyzing Law-Making in Kenya\n",
            "Defining and Evaluating Fair Natural Language Generation\n",
            "Political Advertising Dataset: the use case of the Polish 2020 Presidential Elections\n",
            "The human unlikeness of neural language models in next-word prediction\n",
            "Long-Tail Predictions with Continuous-Output Language Models\n",
            "Analyzing the Framing of 2020 Presidential Candidates in the News\n",
            "Understanding the Impact of Experiment Design for Evaluating Dialogue System Output\n",
            "Studying The Effect of Emotional and Moral Language on Information Contagion during the Charlottesville Event\n",
            "Mapping of Narrative Text Fields To ICD-10 Codes Using Natural Language Processing and Machine Learning\n",
            "Multitask Models for Controlling the Complexity of Neural Machine Translation\n",
            "Using Social Media For Bitcoin Day Trading Behavior Prediction\n",
            "HausaMT v1.0: Towards English–Hausa Neural Machine Translation\n",
            "Outcomes of coming out: Analyzing stories of LGBTQ+\n",
            "An Evaluation of Subword Segmentation Strategies for Neural Machine Translation of Morphologically Rich Languages\n",
            "Enhanced Urdu Word Segmentation using Conditional Random Fields and Morphological Context Features\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# bib to pandas"
      ],
      "metadata": {
        "id": "lChgZRx-L2fZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pybtex"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CaYXPpvPMGro",
        "outputId": "0b5c5663-d88a-4772-de1f-6e3299265827"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pybtex\n",
            "  Downloading pybtex-0.24.0-py2.py3-none-any.whl (561 kB)\n",
            "\u001b[K     |████████████████████████████████| 561 kB 32.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=3.01 in /usr/local/lib/python3.7/dist-packages (from pybtex) (6.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from pybtex) (1.15.0)\n",
            "Collecting latexcodec>=1.0.4\n",
            "  Downloading latexcodec-2.0.1-py2.py3-none-any.whl (18 kB)\n",
            "Installing collected packages: latexcodec, pybtex\n",
            "Successfully installed latexcodec-2.0.1 pybtex-0.24.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install bibtexparser"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c6b9USBbPY7z",
        "outputId": "1ef7dd50-7af1-4acc-f4de-db3460217ea4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting bibtexparser\n",
            "  Downloading bibtexparser-1.3.0-py3-none-any.whl (41 kB)\n",
            "\u001b[K     |████████████████████████████████| 41 kB 398 kB/s \n",
            "\u001b[?25hRequirement already satisfied: pyparsing>=2.0.3 in /usr/local/lib/python3.7/dist-packages (from bibtexparser) (3.0.9)\n",
            "Installing collected packages: bibtexparser\n",
            "Successfully installed bibtexparser-1.3.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import bibtexparser\n"
      ],
      "metadata": {
        "id": "_pMD2P9SPeJp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pybtex.database.input import bibtex\n",
        "\n",
        "#open a bibtex file\n",
        "parser = bibtex.Parser()\n",
        "bibdata = parser.parse_file(\"/content/anthology_abstracts.bib\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 348
        },
        "id": "w_LDxQrxL5f_",
        "outputId": "7c8185ca-60cf-45f6-a80e-8adc89e3fba7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "PrematureEOF",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPrematureEOF\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-111-258505913732>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#open a bibtex file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbibtex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mParser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mbibdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/anthology_abstracts.bib\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pybtex/database/input/__init__.py\u001b[0m in \u001b[0;36mparse_file\u001b[0;34m(self, filename, file_suffix)\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_stream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mUnicodeDecodeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mPybtexError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pybtex/database/input/bibtex.py\u001b[0m in \u001b[0;36mparse_stream\u001b[0;34m(self, stream)\u001b[0m\n\u001b[1;32m    410\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mparse_stream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    411\u001b[0m         \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 412\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pybtex/database/input/bibtex.py\u001b[0m in \u001b[0;36mparse_string\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    397\u001b[0m             \u001b[0mmacros\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmacros\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m         )\n\u001b[0;32m--> 399\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mentry\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentry_iterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    400\u001b[0m             \u001b[0mentry_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mentry\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    401\u001b[0m             \u001b[0mentry_type_lower\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mentry_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pybtex/database/input/bibtex.py\u001b[0m in \u001b[0;36mparse_bibliography\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    193\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mPybtexSyntaxError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mSkipEntry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pybtex/database/input/bibtex.py\u001b[0m in \u001b[0;36mhandle_error\u001b[0;34m(self, error)\u001b[0m\n\u001b[1;32m    383\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mhandle_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mpybtex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrors\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mreport_error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 385\u001b[0;31m         \u001b[0mreport_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    386\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mparse_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pybtex/errors.py\u001b[0m in \u001b[0;36mreport_error\u001b[0;34m(exception)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mstrict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0mprint_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexception\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'WARNING: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pybtex/database/input/bibtex.py\u001b[0m in \u001b[0;36mparse_bibliography\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    191\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommand_start\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m                 \u001b[0;32myield\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    194\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mPybtexSyntaxError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pybtex/database/input/bibtex.py\u001b[0m in \u001b[0;36mparse_command\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    224\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequired\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbody_end\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mPybtexSyntaxError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 226\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    227\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmake_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pybtex/database/input/bibtex.py\u001b[0m in \u001b[0;36mhandle_error\u001b[0;34m(self, error)\u001b[0m\n\u001b[1;32m    383\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mhandle_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mpybtex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrors\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mreport_error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 385\u001b[0;31m         \u001b[0mreport_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    386\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mparse_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pybtex/errors.py\u001b[0m in \u001b[0;36mreport_error\u001b[0;34m(exception)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mstrict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0mprint_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexception\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'WARNING: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pybtex/database/input/bibtex.py\u001b[0m in \u001b[0;36mparse_command\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    221\u001b[0m             \u001b[0mmake_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_entry_key\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_fields\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m             \u001b[0mparse_body\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbody_end\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    224\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequired\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbody_end\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mPybtexSyntaxError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pybtex/database/input/bibtex.py\u001b[0m in \u001b[0;36mparse_entry_body\u001b[0;34m(self, body_end)\u001b[0m\n\u001b[1;32m    240\u001b[0m             \u001b[0mkey_pattern\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mKEY_PAREN\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mbody_end\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRPAREN\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mKEY_BRACE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_entry_key\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequired\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey_pattern\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 242\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_entry_fields\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    243\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwant_current_entry\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mSkipEntry\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pybtex/database/input/bibtex.py\u001b[0m in \u001b[0;36mparse_entry_fields\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    248\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_field_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_field\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_field_name\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_value\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_fields\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_field_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pybtex/database/input/bibtex.py\u001b[0m in \u001b[0;36mparse_field\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_field_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequired\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEQUALS\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 263\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mparse_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pybtex/database/input/bibtex.py\u001b[0m in \u001b[0;36mparse_value\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    272\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstart\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mconcatenation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 274\u001b[0;31m             \u001b[0mvalue_parts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_value_part\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    275\u001b[0m             \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue_parts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pybtex/database/input/bibtex.py\u001b[0m in \u001b[0;36mparse_value_part\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    282\u001b[0m         )\n\u001b[1;32m    283\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpattern\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mQUOTE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 284\u001b[0;31m             \u001b[0mvalue_part\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstring_end\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mQUOTE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    285\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpattern\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLBRACE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m             \u001b[0mvalue_part\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstring_end\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRBRACE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pybtex/database/input/bibtex.py\u001b[0m in \u001b[0;36mflatten_string\u001b[0;34m(self, parts)\u001b[0m\n\u001b[1;32m    292\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mflatten_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 294\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpart\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mpart\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    295\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msubstitute_macro\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pybtex/database/input/bibtex.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    292\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mflatten_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 294\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpart\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mpart\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    295\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msubstitute_macro\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pybtex/database/input/bibtex.py\u001b[0m in \u001b[0;36mparse_string\u001b[0;34m(self, string_end, level, max_level)\u001b[0m\n\u001b[1;32m    318\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mpart\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpattern\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLBRACE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mpart\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 320\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0msubpart\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRBRACE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    321\u001b[0m                     \u001b[0;32myield\u001b[0m \u001b[0msubpart\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mpart\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpattern\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRBRACE\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlevel\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pybtex/database/input/bibtex.py\u001b[0m in \u001b[0;36mparse_string\u001b[0;34m(self, string_end, level, max_level)\u001b[0m\n\u001b[1;32m    318\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mpart\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpattern\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLBRACE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mpart\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 320\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0msubpart\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRBRACE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    321\u001b[0m                     \u001b[0;32myield\u001b[0m \u001b[0msubpart\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mpart\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpattern\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRBRACE\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlevel\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pybtex/database/input/bibtex.py\u001b[0m in \u001b[0;36mparse_string\u001b[0;34m(self, string_end, level, max_level)\u001b[0m\n\u001b[1;32m    318\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mpart\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpattern\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLBRACE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mpart\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 320\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0msubpart\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRBRACE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    321\u001b[0m                     \u001b[0;32myield\u001b[0m \u001b[0msubpart\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mpart\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpattern\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRBRACE\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlevel\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pybtex/database/input/bibtex.py\u001b[0m in \u001b[0;36mparse_string\u001b[0;34m(self, string_end, level, max_level)\u001b[0m\n\u001b[1;32m    318\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mpart\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpattern\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLBRACE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mpart\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 320\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0msubpart\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRBRACE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    321\u001b[0m                     \u001b[0;32myield\u001b[0m \u001b[0msubpart\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mpart\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpattern\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRBRACE\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlevel\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pybtex/database/input/bibtex.py\u001b[0m in \u001b[0;36mparse_string\u001b[0;34m(self, string_end, level, max_level)\u001b[0m\n\u001b[1;32m    318\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mpart\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpattern\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLBRACE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mpart\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 320\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0msubpart\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRBRACE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    321\u001b[0m                     \u001b[0;32myield\u001b[0m \u001b[0msubpart\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mpart\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpattern\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRBRACE\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlevel\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pybtex/database/input/bibtex.py\u001b[0m in \u001b[0;36mparse_string\u001b[0;34m(self, string_end, level, max_level)\u001b[0m\n\u001b[1;32m    318\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mpart\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpattern\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLBRACE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mpart\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 320\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0msubpart\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRBRACE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    321\u001b[0m                     \u001b[0;32myield\u001b[0m \u001b[0msubpart\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mpart\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpattern\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRBRACE\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlevel\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pybtex/database/input/bibtex.py\u001b[0m in \u001b[0;36mparse_string\u001b[0;34m(self, string_end, level, max_level)\u001b[0m\n\u001b[1;32m    318\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mpart\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpattern\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLBRACE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mpart\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 320\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0msubpart\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRBRACE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    321\u001b[0m                     \u001b[0;32myield\u001b[0m \u001b[0msubpart\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mpart\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpattern\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRBRACE\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlevel\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pybtex/database/input/bibtex.py\u001b[0m in \u001b[0;36mparse_string\u001b[0;34m(self, string_end, level, max_level)\u001b[0m\n\u001b[1;32m    318\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mpart\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpattern\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLBRACE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mpart\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 320\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0msubpart\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRBRACE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    321\u001b[0m                     \u001b[0;32myield\u001b[0m \u001b[0msubpart\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mpart\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpattern\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRBRACE\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlevel\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pybtex/database/input/bibtex.py\u001b[0m in \u001b[0;36mparse_string\u001b[0;34m(self, string_end, level, max_level)\u001b[0m\n\u001b[1;32m    318\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mpart\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpattern\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLBRACE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mpart\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 320\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0msubpart\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRBRACE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    321\u001b[0m                     \u001b[0;32myield\u001b[0m \u001b[0msubpart\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mpart\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpattern\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRBRACE\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlevel\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pybtex/database/input/bibtex.py\u001b[0m in \u001b[0;36mparse_string\u001b[0;34m(self, string_end, level, max_level)\u001b[0m\n\u001b[1;32m    318\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mpart\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpattern\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLBRACE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mpart\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 320\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0msubpart\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRBRACE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    321\u001b[0m                     \u001b[0;32myield\u001b[0m \u001b[0msubpart\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mpart\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpattern\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRBRACE\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlevel\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pybtex/database/input/bibtex.py\u001b[0m in \u001b[0;36mparse_string\u001b[0;34m(self, string_end, level, max_level)\u001b[0m\n\u001b[1;32m    318\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mpart\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpattern\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLBRACE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mpart\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 320\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0msubpart\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRBRACE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    321\u001b[0m                     \u001b[0;32myield\u001b[0m \u001b[0msubpart\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mpart\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpattern\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRBRACE\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlevel\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pybtex/database/input/bibtex.py\u001b[0m in \u001b[0;36mparse_string\u001b[0;34m(self, string_end, level, max_level)\u001b[0m\n\u001b[1;32m    318\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mpart\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpattern\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLBRACE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mpart\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 320\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0msubpart\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRBRACE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    321\u001b[0m                     \u001b[0;32myield\u001b[0m \u001b[0msubpart\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mpart\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpattern\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRBRACE\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlevel\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pybtex/database/input/bibtex.py\u001b[0m in \u001b[0;36mparse_string\u001b[0;34m(self, string_end, level, max_level)\u001b[0m\n\u001b[1;32m    318\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mpart\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpattern\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLBRACE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mpart\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 320\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0msubpart\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRBRACE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    321\u001b[0m                     \u001b[0;32myield\u001b[0m \u001b[0msubpart\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mpart\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpattern\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRBRACE\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlevel\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pybtex/database/input/bibtex.py\u001b[0m in \u001b[0;36mparse_string\u001b[0;34m(self, string_end, level, max_level)\u001b[0m\n\u001b[1;32m    312\u001b[0m             \u001b[0mpart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mskip_to\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspecial_chars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mpart\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 314\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mPrematureEOF\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    315\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpart\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpattern\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mstring_end\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mpart\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mPrematureEOF\u001b[0m: syntax error in line 1049501: premature end of file"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "RIb5o0t5QhLO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"/content/anthology+abstracts.bib\") as bibtex_file:\n",
        "    bib_database = bibtexparser.load(bibtex_file)\n",
        "    \n",
        "# df = pd.DataFrame(bib_database.entries)\n",
        "# selection = df[['doi', 'number']]\n",
        "# selection.to_csv('temp.csv', index=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 434
        },
        "id": "WT_C9_pwPMPU",
        "outputId": "80771fa1-89d0-4f50-f96d-b8b2f2ef5795"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "UndefinedString",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/bibtexparser/bibdatabase.py\u001b[0m in \u001b[0;36mexpand_string\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    106\u001b[0m             return BibDataStringExpression.expand_if_expression(\n\u001b[0;32m--> 107\u001b[0;31m                 self.strings[name])\n\u001b[0m\u001b[1;32m    108\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'jul'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mUndefinedString\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-110-c4fdbcea2326>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/anthology+abstracts.bib\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mbibtex_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mbib_database\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbibtexparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbibtex_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# df = pd.DataFrame(bib_database.entries)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# selection = df[['doi', 'number']]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/bibtexparser/__init__.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(bibtex_file, parser)\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mparser\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBibTexParser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbibtex_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/bibtexparser/bparser.py\u001b[0m in \u001b[0;36mparse_file\u001b[0;34m(self, file, partial)\u001b[0m\n\u001b[1;32m    167\u001b[0m         \u001b[0;34m:\u001b[0m\u001b[0mrtype\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mBibDatabase\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m         \"\"\"\n\u001b[0;32m--> 169\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartial\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpartial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_init_expressions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/bibtexparser/bparser.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(self, bibtex_str, partial)\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0mbibtex_file_obj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bibtex_file_obj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbibtex_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_expr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparseFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbibtex_file_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_expr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mParseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Could not parse properly, starting at %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/bibtexparser/bibtexexpression.py\u001b[0m in \u001b[0;36mparseFile\u001b[0;34m(self, file_obj)\u001b[0m\n\u001b[1;32m    276\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mparseFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 278\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmain_expression\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparseFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_obj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparseAll\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyparsing/core.py\u001b[0m in \u001b[0;36mparse_file\u001b[0;34m(self, file_or_filename, encoding, parse_all, parseAll)\u001b[0m\n\u001b[1;32m   1905\u001b[0m                 \u001b[0mfile_contents\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1906\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1907\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_contents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparseAll\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1908\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mParseBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1909\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mParserElement\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose_stacktrace\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyparsing/core.py\u001b[0m in \u001b[0;36mparse_string\u001b[0;34m(self, instring, parse_all, parseAll)\u001b[0m\n\u001b[1;32m   1129\u001b[0m             \u001b[0minstring\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minstring\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpandtabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1130\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1131\u001b[0;31m             \u001b[0mloc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1132\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mparseAll\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1133\u001b[0m                 \u001b[0mloc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreParse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyparsing/core.py\u001b[0m in \u001b[0;36m_parseCache\u001b[0;34m(self, instring, loc, doActions, callPreParse)\u001b[0m\n\u001b[1;32m    919\u001b[0m                 \u001b[0mParserElement\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpackrat_cache_stats\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mMISS\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    920\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 921\u001b[0;31m                     \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parseNoCache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoActions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallPreParse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    922\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mParseBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpe\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    923\u001b[0m                     \u001b[0;31m# cache a copy of the exception, without the traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyparsing/core.py\u001b[0m in \u001b[0;36m_parseNoCache\u001b[0;34m(self, instring, loc, doActions, callPreParse)\u001b[0m\n\u001b[1;32m    815\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmayIndexError\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mpre_loc\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mlen_instring\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    816\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 817\u001b[0;31m                     \u001b[0mloc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparseImpl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpre_loc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoActions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    818\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    819\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mParseException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen_instring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyparsing/core.py\u001b[0m in \u001b[0;36mparseImpl\u001b[0;34m(self, instring, loc, doActions)\u001b[0m\n\u001b[1;32m   4889\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mparseImpl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minstring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoActions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4890\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4891\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparseImpl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoActions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4892\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mParseException\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4893\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mloc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mParseResults\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresultsName\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyparsing/core.py\u001b[0m in \u001b[0;36mparseImpl\u001b[0;34m(self, instring, loc, doActions)\u001b[0m\n\u001b[1;32m   4788\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcheck_ender\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4789\u001b[0m             \u001b[0mtry_not_ender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4790\u001b[0;31m         \u001b[0mloc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself_expr_parse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoActions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4791\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4792\u001b[0m             \u001b[0mhasIgnoreExprs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mignoreExprs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyparsing/core.py\u001b[0m in \u001b[0;36m_parseCache\u001b[0;34m(self, instring, loc, doActions, callPreParse)\u001b[0m\n\u001b[1;32m    919\u001b[0m                 \u001b[0mParserElement\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpackrat_cache_stats\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mMISS\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    920\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 921\u001b[0;31m                     \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parseNoCache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoActions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallPreParse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    922\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mParseBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpe\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    923\u001b[0m                     \u001b[0;31m# cache a copy of the exception, without the traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyparsing/core.py\u001b[0m in \u001b[0;36m_parseNoCache\u001b[0;34m(self, instring, loc, doActions, callPreParse)\u001b[0m\n\u001b[1;32m    815\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmayIndexError\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mpre_loc\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mlen_instring\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    816\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 817\u001b[0;31m                     \u001b[0mloc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparseImpl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpre_loc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoActions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    818\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    819\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mParseException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen_instring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyparsing/core.py\u001b[0m in \u001b[0;36mparseImpl\u001b[0;34m(self, instring, loc, doActions)\u001b[0m\n\u001b[1;32m   4115\u001b[0m                     \u001b[0minstring\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4116\u001b[0m                     \u001b[0mloc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4117\u001b[0;31m                     \u001b[0mdoActions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4118\u001b[0m                 )\n\u001b[1;32m   4119\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mParseFatalException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpfe\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyparsing/core.py\u001b[0m in \u001b[0;36m_parseCache\u001b[0;34m(self, instring, loc, doActions, callPreParse)\u001b[0m\n\u001b[1;32m    919\u001b[0m                 \u001b[0mParserElement\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpackrat_cache_stats\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mMISS\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    920\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 921\u001b[0;31m                     \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parseNoCache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoActions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallPreParse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    922\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mParseBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpe\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    923\u001b[0m                     \u001b[0;31m# cache a copy of the exception, without the traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyparsing/core.py\u001b[0m in \u001b[0;36m_parseNoCache\u001b[0;34m(self, instring, loc, doActions, callPreParse)\u001b[0m\n\u001b[1;32m    854\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mfn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparseAction\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    855\u001b[0m                     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 856\u001b[0;31m                         \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokens_start\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mret_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    857\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mIndexError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mparse_action_exc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m                         \u001b[0mexc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mParseException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"exception raised in parse action\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyparsing/core.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    289\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 291\u001b[0;31m                 \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlimit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    292\u001b[0m                 \u001b[0mfound_arity\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/bibtexparser/bparser.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(s, l, t)\u001b[0m\n\u001b[1;32m    186\u001b[0m         self._expr.entry.addParseAction(\n\u001b[1;32m    187\u001b[0m             lambda s, l, t: self._add_entry(\n\u001b[0;32m--> 188\u001b[0;31m                 t.get('EntryType'), t.get('Key'), t.get('Fields'))\n\u001b[0m\u001b[1;32m    189\u001b[0m             )\n\u001b[1;32m    190\u001b[0m         self._expr.implicit_comment.addParseAction(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/bibtexparser/bparser.py\u001b[0m in \u001b[0;36m_add_entry\u001b[0;34m(self, entry_type, entry_id, fields)\u001b[0m\n\u001b[1;32m    275\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfields\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 277\u001b[0;31m             \u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_clean_field_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_clean_val\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfields\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m         \u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ENTRYTYPE'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mentry_type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m         \u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ID'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mentry_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/bibtexparser/bparser.py\u001b[0m in \u001b[0;36m_clean_val\u001b[0;34m(self, val)\u001b[0m\n\u001b[1;32m    226\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpolate_strings\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 228\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    229\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/bibtexparser/bibdatabase.py\u001b[0m in \u001b[0;36mas_text\u001b[0;34m(text_string_or_expression)\u001b[0m\n\u001b[1;32m    268\u001b[0m     if isinstance(text_string_or_expression,\n\u001b[1;32m    269\u001b[0m                   (BibDataString, BibDataStringExpression)):\n\u001b[0;32m--> 270\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtext_string_or_expression\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    271\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_string_or_expression\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/bibtexparser/bibdatabase.py\u001b[0m in \u001b[0;36mget_value\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    229\u001b[0m         \u001b[0;34m:\u001b[0m\u001b[0mreturns\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m         \"\"\"\n\u001b[0;32m--> 231\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mBibDataString\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    232\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_on_strings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/bibtexparser/bibdatabase.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    229\u001b[0m         \u001b[0;34m:\u001b[0m\u001b[0mreturns\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m         \"\"\"\n\u001b[0;32m--> 231\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mBibDataString\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    232\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_on_strings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/bibtexparser/bibdatabase.py\u001b[0m in \u001b[0;36mexpand_string\u001b[0;34m(string_or_bibdatastring)\u001b[0m\n\u001b[1;32m    195\u001b[0m         \"\"\"\n\u001b[1;32m    196\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstring_or_bibdatastring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBibDataString\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mstring_or_bibdatastring\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    198\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mstring_or_bibdatastring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/bibtexparser/bibdatabase.py\u001b[0m in \u001b[0;36mget_value\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    176\u001b[0m         \u001b[0;34m:\u001b[0m\u001b[0mreturns\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m         \"\"\"\n\u001b[0;32m--> 178\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bibdatabase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_dependencies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mknown_dependencies\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/bibtexparser/bibdatabase.py\u001b[0m in \u001b[0;36mexpand_string\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    107\u001b[0m                 self.strings[name])\n\u001b[1;32m    108\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m             \u001b[0;32mraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mUndefinedString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_add_missing_from_crossref_entry\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mentry\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdependencies\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mUndefinedString\u001b[0m: 'jul'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#loop through the individual references\n",
        "for bib_id in bibdata.entries:\n",
        "    b = bibdata.entries[bib_id].fields\n",
        "    try:\n",
        "        # change these lines to create a SQL insert\n",
        "        print(b[\"title\"])\n",
        "        print(b[\"abstract\"])\n",
        "        print(b[\"year\"])\n",
        "        #deal with multiple authors\n",
        "        # for author in bibdata.entries[bib_id].persons[\"author\"]:\n",
        "        #     print author.first(), author.last()\n",
        "    # field may not exist for a reference\n",
        "    except(KeyError):\n",
        "        continue"
      ],
      "metadata": {
        "id": "ZE970dX1MeCJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/anthology_abstracts.bib') as bibtex_file:\n",
        "  bib_database = bibtexparser.load(bibtex_file)\n",
        "  \n",
        "df = pd.DataFrame(bib_database.entries)\n",
        "df.to_csv('ref.csv', index=False)"
      ],
      "metadata": {
        "id": "Elno9BLzQ1bo",
        "outputId": "b6cc5eaa-5620-40b9-bf6c-fb0921015a03",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 434
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "UndefinedString",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/bibtexparser/bibdatabase.py\u001b[0m in \u001b[0;36mexpand_string\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    106\u001b[0m             return BibDataStringExpression.expand_if_expression(\n\u001b[0;32m--> 107\u001b[0;31m                 self.strings[name])\n\u001b[0m\u001b[1;32m    108\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'jul'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mUndefinedString\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-112-7bb39fba6e73>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/anthology_abstracts.bib'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mbibtex_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m   \u001b[0mbib_database\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbibtexparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbibtex_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbib_database\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mentries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ref.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/bibtexparser/__init__.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(bibtex_file, parser)\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mparser\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBibTexParser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbibtex_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/bibtexparser/bparser.py\u001b[0m in \u001b[0;36mparse_file\u001b[0;34m(self, file, partial)\u001b[0m\n\u001b[1;32m    167\u001b[0m         \u001b[0;34m:\u001b[0m\u001b[0mrtype\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mBibDatabase\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m         \"\"\"\n\u001b[0;32m--> 169\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartial\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpartial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_init_expressions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/bibtexparser/bparser.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(self, bibtex_str, partial)\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0mbibtex_file_obj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bibtex_file_obj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbibtex_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_expr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparseFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbibtex_file_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_expr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mParseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Could not parse properly, starting at %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/bibtexparser/bibtexexpression.py\u001b[0m in \u001b[0;36mparseFile\u001b[0;34m(self, file_obj)\u001b[0m\n\u001b[1;32m    276\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mparseFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 278\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmain_expression\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparseFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_obj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparseAll\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyparsing/core.py\u001b[0m in \u001b[0;36mparse_file\u001b[0;34m(self, file_or_filename, encoding, parse_all, parseAll)\u001b[0m\n\u001b[1;32m   1905\u001b[0m                 \u001b[0mfile_contents\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1906\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1907\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_contents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparseAll\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1908\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mParseBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1909\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mParserElement\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose_stacktrace\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyparsing/core.py\u001b[0m in \u001b[0;36mparse_string\u001b[0;34m(self, instring, parse_all, parseAll)\u001b[0m\n\u001b[1;32m   1129\u001b[0m             \u001b[0minstring\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minstring\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpandtabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1130\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1131\u001b[0;31m             \u001b[0mloc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1132\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mparseAll\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1133\u001b[0m                 \u001b[0mloc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreParse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyparsing/core.py\u001b[0m in \u001b[0;36m_parseCache\u001b[0;34m(self, instring, loc, doActions, callPreParse)\u001b[0m\n\u001b[1;32m    919\u001b[0m                 \u001b[0mParserElement\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpackrat_cache_stats\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mMISS\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    920\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 921\u001b[0;31m                     \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parseNoCache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoActions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallPreParse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    922\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mParseBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpe\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    923\u001b[0m                     \u001b[0;31m# cache a copy of the exception, without the traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyparsing/core.py\u001b[0m in \u001b[0;36m_parseNoCache\u001b[0;34m(self, instring, loc, doActions, callPreParse)\u001b[0m\n\u001b[1;32m    815\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmayIndexError\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mpre_loc\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mlen_instring\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    816\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 817\u001b[0;31m                     \u001b[0mloc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparseImpl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpre_loc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoActions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    818\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    819\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mParseException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen_instring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyparsing/core.py\u001b[0m in \u001b[0;36mparseImpl\u001b[0;34m(self, instring, loc, doActions)\u001b[0m\n\u001b[1;32m   4889\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mparseImpl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minstring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoActions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4890\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4891\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparseImpl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoActions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4892\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mParseException\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4893\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mloc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mParseResults\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresultsName\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyparsing/core.py\u001b[0m in \u001b[0;36mparseImpl\u001b[0;34m(self, instring, loc, doActions)\u001b[0m\n\u001b[1;32m   4788\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcheck_ender\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4789\u001b[0m             \u001b[0mtry_not_ender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4790\u001b[0;31m         \u001b[0mloc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself_expr_parse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoActions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4791\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4792\u001b[0m             \u001b[0mhasIgnoreExprs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mignoreExprs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyparsing/core.py\u001b[0m in \u001b[0;36m_parseCache\u001b[0;34m(self, instring, loc, doActions, callPreParse)\u001b[0m\n\u001b[1;32m    919\u001b[0m                 \u001b[0mParserElement\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpackrat_cache_stats\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mMISS\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    920\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 921\u001b[0;31m                     \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parseNoCache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoActions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallPreParse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    922\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mParseBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpe\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    923\u001b[0m                     \u001b[0;31m# cache a copy of the exception, without the traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyparsing/core.py\u001b[0m in \u001b[0;36m_parseNoCache\u001b[0;34m(self, instring, loc, doActions, callPreParse)\u001b[0m\n\u001b[1;32m    815\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmayIndexError\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mpre_loc\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mlen_instring\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    816\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 817\u001b[0;31m                     \u001b[0mloc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparseImpl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpre_loc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoActions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    818\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    819\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mParseException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen_instring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyparsing/core.py\u001b[0m in \u001b[0;36mparseImpl\u001b[0;34m(self, instring, loc, doActions)\u001b[0m\n\u001b[1;32m   4115\u001b[0m                     \u001b[0minstring\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4116\u001b[0m                     \u001b[0mloc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4117\u001b[0;31m                     \u001b[0mdoActions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4118\u001b[0m                 )\n\u001b[1;32m   4119\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mParseFatalException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpfe\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyparsing/core.py\u001b[0m in \u001b[0;36m_parseCache\u001b[0;34m(self, instring, loc, doActions, callPreParse)\u001b[0m\n\u001b[1;32m    919\u001b[0m                 \u001b[0mParserElement\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpackrat_cache_stats\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mMISS\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    920\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 921\u001b[0;31m                     \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parseNoCache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoActions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallPreParse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    922\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mParseBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpe\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    923\u001b[0m                     \u001b[0;31m# cache a copy of the exception, without the traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyparsing/core.py\u001b[0m in \u001b[0;36m_parseNoCache\u001b[0;34m(self, instring, loc, doActions, callPreParse)\u001b[0m\n\u001b[1;32m    854\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mfn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparseAction\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    855\u001b[0m                     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 856\u001b[0;31m                         \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokens_start\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mret_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    857\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mIndexError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mparse_action_exc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m                         \u001b[0mexc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mParseException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"exception raised in parse action\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyparsing/core.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    289\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 291\u001b[0;31m                 \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlimit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    292\u001b[0m                 \u001b[0mfound_arity\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/bibtexparser/bparser.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(s, l, t)\u001b[0m\n\u001b[1;32m    186\u001b[0m         self._expr.entry.addParseAction(\n\u001b[1;32m    187\u001b[0m             lambda s, l, t: self._add_entry(\n\u001b[0;32m--> 188\u001b[0;31m                 t.get('EntryType'), t.get('Key'), t.get('Fields'))\n\u001b[0m\u001b[1;32m    189\u001b[0m             )\n\u001b[1;32m    190\u001b[0m         self._expr.implicit_comment.addParseAction(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/bibtexparser/bparser.py\u001b[0m in \u001b[0;36m_add_entry\u001b[0;34m(self, entry_type, entry_id, fields)\u001b[0m\n\u001b[1;32m    275\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfields\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 277\u001b[0;31m             \u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_clean_field_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_clean_val\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfields\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m         \u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ENTRYTYPE'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mentry_type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m         \u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ID'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mentry_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/bibtexparser/bparser.py\u001b[0m in \u001b[0;36m_clean_val\u001b[0;34m(self, val)\u001b[0m\n\u001b[1;32m    226\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpolate_strings\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 228\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    229\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/bibtexparser/bibdatabase.py\u001b[0m in \u001b[0;36mas_text\u001b[0;34m(text_string_or_expression)\u001b[0m\n\u001b[1;32m    268\u001b[0m     if isinstance(text_string_or_expression,\n\u001b[1;32m    269\u001b[0m                   (BibDataString, BibDataStringExpression)):\n\u001b[0;32m--> 270\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtext_string_or_expression\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    271\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_string_or_expression\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/bibtexparser/bibdatabase.py\u001b[0m in \u001b[0;36mget_value\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    229\u001b[0m         \u001b[0;34m:\u001b[0m\u001b[0mreturns\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m         \"\"\"\n\u001b[0;32m--> 231\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mBibDataString\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    232\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_on_strings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/bibtexparser/bibdatabase.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    229\u001b[0m         \u001b[0;34m:\u001b[0m\u001b[0mreturns\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m         \"\"\"\n\u001b[0;32m--> 231\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mBibDataString\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    232\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_on_strings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/bibtexparser/bibdatabase.py\u001b[0m in \u001b[0;36mexpand_string\u001b[0;34m(string_or_bibdatastring)\u001b[0m\n\u001b[1;32m    195\u001b[0m         \"\"\"\n\u001b[1;32m    196\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstring_or_bibdatastring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBibDataString\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mstring_or_bibdatastring\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    198\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mstring_or_bibdatastring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/bibtexparser/bibdatabase.py\u001b[0m in \u001b[0;36mget_value\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    176\u001b[0m         \u001b[0;34m:\u001b[0m\u001b[0mreturns\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m         \"\"\"\n\u001b[0;32m--> 178\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bibdatabase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_dependencies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mknown_dependencies\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/bibtexparser/bibdatabase.py\u001b[0m in \u001b[0;36mexpand_string\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    107\u001b[0m                 self.strings[name])\n\u001b[1;32m    108\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m             \u001b[0;32mraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mUndefinedString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_add_missing_from_crossref_entry\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mentry\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdependencies\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mUndefinedString\u001b[0m: 'jul'"
          ]
        }
      ]
    }
  ]
}